[[archivos/bibliografia/2-Sistemas operativos moderno 3ed Tanenbaum.pdf|2-Sistemas operativos moderno 3ed Tanenbaum]]

Escrito por **Adrián Quiroga Linares**.

# Procesos
En sistemas de **multiprogramación**, la CPU alterna rápidamente entre procesos, ejecutando cada uno durante breves periodos. Esto crea la ilusión de **paralelismo** (o **pseudoparalelismo**) en sistemas con una sola CPU. En contraste, los sistemas **multiprocesadores** logran **paralelismo real** mediante varias CPUs que comparten memoria física.

Para simplificar el manejo del paralelismo, los sistemas operativos han adoptado un modelo basado en **procesos secuenciales**, facilitando el diseño y administración de múltiples tareas concurrentes. Este modelo y sus implicaciones son el enfoque principal del capítulo.

## El modelo del proceso
### **¿Qué es un proceso?**
Un proceso es una **instancia de un programa en ejecución**, incluyendo:
- **Contador de programa lógico:** Indica la próxima instrucción a ejecutar.
- **Registros y variables:** Contienen los datos que el proceso necesita.
- **Estado:** Describe si el proceso está en **ejecución, esperando, o detenido**.

Aunque físicamente solo una CPU real ejecuta instrucciones a la vez, el sistema operativo con **multiprogramación** alterna entre procesos de manera rápida. Esto crea la **ilusión de ejecución paralela** o "pseudoparalelismo". 

### **Multiprogramación y conmutación de procesos**
En un sistema de multiprogramación:
1. La CPU **ejecuta** un proceso durante un breve período.
2. **Guarda el estado** del proceso actual (incluyendo el contador de programa).
3. **Carga el estado de otro proceso y continúa su ejecución**.

El cambio entre procesos permite **aprovechar los tiempos muertos**, como cuando un proceso espera una operación de entrada/salida. En un período suficiente, todos los procesos progresan, pero **en cualquier instante solo uno está activo**.

![[archivos/imagenes/Pasted image 20241117162520.png]]

### Diferencia entre programa y proceso
La diferencia clave es que:
- **Programa:** Es **estático**, un conjunto de instrucciones **almacenadas** (por ejemplo, una receta de cocina).
- **Proceso:** Es **dinámico**, la actividad de ejecutar un programa con entradas y salidas específicas (como seguir la receta para hornear un pastel).

**Ejemplo:**  
Un científico sigue una receta para hacer un pastel (proceso 1). Si ocurre una emergencia (su hijo es picado por una abeja), el científico guarda su progreso, atiende al niño siguiendo un libro de primeros auxilios (proceso 2), y luego vuelve al pastel. Este ejemplo muestra:
- **Conmutación de procesos:** La CPU cambia de un proceso a otro según prioridades.
- **Estado guardado y restaurado:** Permite continuar desde donde se dejó.

### **Importancia de los procesos**
El modelo del proceso simplifica la gestión de tareas concurrentes en un sistema operativo, permitiendo:
- **Ejecutar múltiples aplicaciones simultáneamente**.
- **Compartir recursos de manera eficiente**.
- **Gestionar prioridades y garantizar que los procesos importantes reciban atención**.

En resumen, un **proceso** es la actividad de ejecutar un programa con un estado y recursos propios. Gracias al modelo de procesos, los sistemas operativos modernos pueden manejar múltiples tareas a la vez, dando la impresión de paralelismo incluso en sistemas con una sola CPU.

## Creación de un proceso
La **creación de un proceso** es un paso esencial en cualquier sistema operativo, ya que permite la ejecución de programas. Dependiendo del tipo de sistema, los procesos pueden ser creados de diversas maneras. En sistemas simples o de propósito específico, los procesos se crean al inicio, mientras que en sistemas más complejos, como los sistemas operativos de propósito general, los procesos pueden crearse y terminarse en cualquier momento durante su ejecución.

1. **Arranque del sistema**: Cuando el sistema operativo arranca, se crean varios procesos, algunos para tareas en **primer plano** (*interactúan con el usuario*) y otros en **segundo plano** (*funciones del sistema como gestión de correo electrónico o páginas web*). Estos últimos se conocen como **demonios** y trabajan en segundo plano desde el arranque del ordenador hasta apagarlo. Para listar los procesos que se están ejecutando usamos `ps`.
   
2. **Ejecutar una llamada al sistema desde un proceso existente**: Un proceso en ejecución puede **crear nuevos procesos para ayudarlo a realizar una tarea más eficiente**. Por ejemplo, si se necesita obtener grandes cantidades de datos, un proceso puede crear un nuevo proceso para descargar los datos mientras otro los procesa. En unix se usa `fork()`

3. **Petición del usuario**: Los usuarios pueden crear procesos al **ejecutar programas desde la interfaz**, ya sea mediante un comando o haciendo clic en un ícono. **Esto generalmente lanza un nuevo proceso** que se ejecuta en su propia ventana.

4. **Inicio de un trabajo por lotes**: En sistemas de procesamiento por lotes, como en mainframes, los trabajos pueden ser enviados en cola y, cuando el sistema tiene recursos disponibles, se crea un proceso para ejecutar el siguiente trabajo de la cola.

La creación de un proceso generalmente ocurre a través de una **llamada al sistema** que indica al sistema operativo que debe crear un nuevo proceso. La forma en que esto se implementa varía según el sistema operativo.

- **En UNIX**: Se utiliza una llamada al sistema llamada **`fork`**. Esta función crea un proceso hijo, que es un clon exacto del proceso que hizo la llamada. Luego, el proceso hijo puede utilizar **`execve`** (*o una llamada similar*) para reemplazar su imagen de memoria y ejecutar un nuevo programa. Esto permite que el proceso hijo realice una tarea diferente de su proceso padre.
  
  - El proceso **padre** y el **proceso hijo** comparten algunos recursos como **archivos abiertos**, pero cada uno tiene su **propio** **espacio de direcciones**, lo que significa que **no comparten memoria modificable**. El proceso hijo comienza con una copia del espacio de direcciones del padre.

## Terminación de procesos
La **terminación de un proceso** es el paso final de su ciclo de vida. Los procesos concluyen por diversas razones, y su finalización puede ser **voluntaria** o **involuntaria**, dependiendo de las circunstancias que enfrenten. Las principales causas de terminación de procesos son las siguientes:


La mayoría de los procesos terminan porque **completan el trabajo que se les asignó**. Por ejemplo:
- Un compilador finaliza después de compilar un programa y llama al sistema operativo para señalar su conclusión (*mediante la llamada al sistema `exit` o `return` en UNIX*).


Un proceso puede decidir terminar cuando **detecta un error que le impide continuar**, como:
- Intentar abrir un archivo que no existe (por ejemplo, al compilar un archivo no encontrado como `gcc foo.c`).
- Aunque algunos programas, especialmente los interactivos, manejan los errores pidiendo al usuario que corrija la entrada (*por ejemplo, mostrando un cuadro de diálogo*), otros simplemente se detienen.


Los errores fatales suelen ser causados por fallas graves en el programa. Ejemplos incluyen:
- **Ejecutar una instrucción ilegal**, como intentar usar una operación no soportada por la CPU.
- **Acceso no válido a memoria**, como intentar leer o escribir en una dirección fuera del espacio asignado.
- **División entre cero**, que no tiene un resultado definido.
  
En algunos sistemas, como UNIX, los procesos pueden registrarse para manejar ciertos errores (*por ejemplo, señales como `SIGSEGV` para fallos de segmentación*). En lugar de terminar de inmediato, el sistema operativo envía una **señal** al proceso para que intente gestionar el error antes de detenerse.


Un proceso puede **ser terminado por otro proceso** mediante una llamada al sistema. Esto puede suceder si:
- El proceso **eliminador decide que otro proceso debe finalizar** (por ejemplo, porque está consumiendo demasiados recursos o está bloqueado).
- **En UNIX, la llamada al sistema utilizada es** **`kill`**.

La terminación de un proceso incluye la **liberación de los recursos que estaba** utilizando, como memoria, descriptores de archivo y entradas en las tablas del sistema operativo.

## **Jerarquías de procesos**

Una **jerarquía de procesos** se da cuando un proceso (padre) crea otros procesos (hijos), y estos, a su vez, pueden crear sus propios descendientes. Esto genera una estructura similar a un árbol. Aunque un proceso puede tener múltiples hijos, **siempre tiene un único padre**. 

La relación padre-hijo puede implicar ciertas dependencias o formas de asociación, que varían según el sistema operativo.

En UNIX, un proceso y sus hijos forman un **grupo de procesos**. 
  1. **Proceso raíz:** Es el proceso inicial llamado **`init`** (o su equivalente en sistemas modernos como `systemd`), que se inicia al encender el sistema.
  2. **Hijos de `init`:** `init` lee un archivo de configuración para identificar las terminales conectadas y crea un proceso para cada una. Cada terminal espera inicios de sesión.
  3. **Cascada:** Cuando un usuario inicia sesión, se ejecuta un **shell**, que puede lanzar nuevos procesos, formando un árbol donde **`init`** es la raíz.

En **windows no existe esta jerarquía**.

## **Estados de un proceso**
### **¿Qué son los estados de un proceso?**
Un **proceso** pasa por diferentes estados durante su ciclo de vida dependiendo de las tareas que realiza y de los recursos disponibles en el sistema. Los tres estados principales son:

1. **Ejecución:**
   - El proceso está utilizando la CPU activamente en ese momento.
2. **Listo:**
   - El proceso está preparado para ejecutarse, pero espera su turno porque la CPU está ocupada con otro proceso.
3. **Bloqueado:**
   - El proceso no puede continuar porque necesita que ocurra un evento externo (por ejemplo, la llegada de datos de entrada).

![[archivos/imagenes/Pasted image 20241209105213.png]]

> [!Info] **Suspendido-> en memoria secundaria**
> Motivos para la suspension de un proceso:
![[archivos/imagenes/Pasted image 20241209105515.png]]

### **Transiciones entre estados:**
El movimiento de un proceso entre estos estados se describe mediante un **diagrama de estados**, donde se identifican cuatro transiciones principales:

1. **De ejecución a bloqueado:**
   - Ocurre cuando el proceso necesita esperar un evento, como datos de entrada. Por ejemplo:
     - Leer desde un archivo que aún no está disponible.
     - Esperar la respuesta de una red.
   - En UNIX, esto puede suceder automáticamente cuando un proceso intenta leer datos y no hay nada disponible.

2. **De ejecución a listo:**
   - El planificador decide que el proceso ha usado suficiente tiempo de CPU y lo sustituye por otro proceso. Este cambio ocurre sin que el proceso lo perciba.

3. **De listo a ejecución:**
   - El planificador selecciona el proceso para que utilice la CPU nuevamente porque ha llegado su turno.

4. **De bloqueado a listo:**
   - Ocurre cuando el evento externo que esperaba el proceso (por ejemplo, la llegada de datos) se ha producido. Si no hay otro proceso en ejecución, este puede pasar inmediatamente a ejecución; de lo contrario, debe esperar en la cola de procesos listos.

![[archivos/imagenes/Pasted image 20241117175251.png]]

## Tabla de procesos
La **tabla de procesos** es una estructura de datos mantenida por el sistema operativo, que almacena información sobre todos los procesos en ejecución o en espera en el sistema. Esta tabla es fundamental para la gestión de procesos, ya que organiza y guarda datos necesarios para el seguimiento, la programación y la ejecución de los procesos.

![[archivos/imagenes/Pasted image 20241117183104.png]]


![[archivos/imagenes/Pasted image 20241216202218.png]]

**Principales contenidos de la tabla de procesos**

![[archivos/imagenes/Pasted image 20241216202314.png]]

Con cada clase de **E/S** hay una ubicación asociada, a la cual se le llama **vector de interrupción**. Esta ubicación contiene contiene la dirección del procedimiento del **servicio de interrupciones**. 

Si está corriendo un proceso y ocurre una **interrupción**, el **PC, SW y registros** del proceso se meten en la pila. Después el ordenador pasa a la dirección especificada en el **vector de interrupción**.

![[archivos/imagenes/Pasted image 20241217094018.png]]

Todas las **interrupciones** guardan los registros en la entrada de la tabla de procesos del proceso actual. Después, se quita la información que la **interrupción metió en la pila** y el apuntador de pila se establece para que **apunte a una pila temporal utilizada por el manejador de procesos**.

Cuando termina esta rutina, llama a un procedimiento en C para realizar el resto del trabajo para este tipo de interrupción específico. Cuando ha terminado su trabajo y tal vez ocasionando que algún otro proceso esté entonces listo, **el planificador es llamado para ver qué proceso se debe ejecutar a continuación**.

![[archivos/imagenes/Pasted image 20241216203859.png]]

# Hilos
En los sistemas operativos tradicionales, cada proceso tiene su propio **espacio de direcciones** y un solo **hilo de control**. Sin embargo, en algunos casos, resulta más conveniente tener **varios hilos dentro del mismo proceso**, lo que permite ejecutar tareas en paralelo dentro del mismo espacio de direcciones, lo que facilita el diseño de ciertas aplicaciones.
#### 1. **Simplicidad en el modelo de programación**
Cuando se descompone una aplicación en varios hilos, el modelo de programación se simplifica. En lugar de pensar en **interrupciones** y **conmutaciones de contexto**, que son típicos de los procesos, se puede pensar en **procesos paralelos**. Los hilos permiten que varias tareas se ejecuten en paralelo y, lo más importante, compartan el mismo **espacio de direcciones** (*es decir, accedan a los mismos datos*). Esto no sería posible si se usaran varios procesos, ya que cada proceso tiene su propio espacio de direcciones.

#### 2. **Eficiencia en la creación y destrucción**
Los hilos son **mucho más ligeros que los procesos**, lo que significa que son más rápidos de crear y destruir. En muchos sistemas, crear un hilo puede ser **de 10 a 100 veces más rápido que crear un proceso**. Esto es muy útil cuando el número de hilos que se necesitan cambia de forma dinámica y rápida.

#### 3. **Mejora en el rendimiento**
Aunque los hilos no aumentan el rendimiento cuando todos están limitados por la CPU, son muy útiles en situaciones donde hay operaciones de **entrada/salida (E/S)** o cálculos intensivos. Al tener hilos, las operaciones de E/S y los cálculos pueden realizarse en paralelo, **traslapándose**, lo que mejora el rendimiento de la aplicación.

#### 4. **Paralelismo real en sistemas multiprocesador**
Cuando se ejecutan aplicaciones en sistemas con varias CPUs (procesadores), los hilos permiten aprovechar el verdadero **paralelismo**, ya que cada hilo puede ejecutarse en diferentes CPUs al mismo tiempo, maximizando el uso de los recursos disponibles.

![[archivos/imagenes/Pasted image 20241217094823.png]]

## Modelo clásico de hilo
El **modelo clásico de hilos** describe cómo los hilos permiten la ejecución concurrente dentro de un solo proceso. Un proceso es una entidad que agrupa recursos como espacio de direcciones, archivos abiertos, y otros recursos del sistema, mientras que un hilo se encarga de ejecutar el código. Aquí te explico los puntos clave:

1. **Separación de recursos y ejecución**: El modelo de procesos tradicional agrupa recursos (*como memoria y archivos*) y la ejecución (*hilos*). Los hilos permiten que varios flujos de ejecución se compartan en un mismo proceso, pero **cada uno de ellos tiene su propio contador de programa, pila y registros**, permitiendo que se realicen tareas concurrentemente dentro del mismo espacio de direcciones.

![[archivos/imagenes/Pasted image 20241117191436.png]]

![[archivos/imagenes/Pasted image 20241117191416.png]]

2. **Ejecución concurrente dentro de un proceso**: Cuando hay múltiples hilos dentro de un solo proceso, todos **comparten el mismo espacio de direcciones**, pero cada uno puede ejecutar una parte del código de manera independiente. Es como tener varios procesos, pero **sin tener que duplicar los recursos**.
>[!Importante]
>**Los hilos no compiten, COOPERAN**

![[archivos/imagenes/Pasted image 20241117191405.png]]

3. **Comparación con procesos**: A diferencia de los procesos, que están aislados en cuanto a sus espacios de direcciones y recursos, los hilos dentro de un proceso **comparten la memoria, archivos abiertos, señales, etc**. Esto significa que los hilos dentro de un mismo proceso pueden comunicarse fácilmente entre sí y colaborar en tareas sin necesidad de mecanismos complicados de comunicación entre procesos.

4. **Propiedades de los hilos**: Los hilos tienen un **contador de programa** (*que marca qué instrucción ejecutar a continuación*), **registros** (*para almacenar variables locales*), y una **pila** (*para gestionar el historial de llamadas de función*). Aunque varios hilos pueden compartir la misma memoria, cada hilo tiene su propia pila, para almacenar las variables locales de las funciones en ejecución.

5. **Conmutación de hilos**: Al igual que en la multiprogramación de procesos, la CPU **cambia rápidamente** entre los hilos para dar la impresión de que se están ejecutando en paralelo. Este tipo de ejecución es más eficiente que tener múltiples procesos completos corriendo, ya que los hilos comparten recursos, lo que reduce la sobrecarga de gestión de memoria.

6. **Estado de los hilos**: Los hilos pueden estar en distintos estados, como "en ejecución", "bloqueado" (*esperando un evento, como la lectura de un archivo*), o "listo" (*esperando su turno para ejecutarse*). Esto es similar al ciclo de vida de los procesos, pero con la particularidad de que todos los hilos comparten los mismos recursos.

7. **Interacción y sincronización entre hilos**: Como los hilos comparten recursos, es importante tener mecanismos para evitar problemas como el acceso concurrente a datos compartidos (por ejemplo, **un hilo cerrando un archivo mientras otro lo lee**). Las herramientas de sincronización, como los semáforos o los mutexes, son necesarias para evitar estos problemas.

8. **Desventajas y complicaciones**: Aunque los hilos son útiles, pueden generar complicaciones. Por ejemplo, si un proceso con varios hilos crea un nuevo proceso, ¿deben los hilos del padre ser duplicados en el hijo? También surgen problemas de sincronización, como si dos hilos intentan acceder a la misma memoria al mismo tiempo. Estos problemas deben resolverse mediante técnicas de programación cuidadosa.

> [!Importante]
> **VARIABLES:**
> - **Accesibles por todos los hilos**:
>    - Variables globales
>    - Archivos abiertos
>    - Memoria dinámica (heap)
>    - Otros recursos compartidos del sistema (como señales)
> - **Privadas a cada hilo**:
>    - Variables locales
>    - Pila del hilo (incluyendo el contexto de ejecución y las variables locales de las funciones)


Cuando se utiliza el modelo de **multihilamiento**, los procesos generalmente comienzan con **un solo hilo**. Este hilo inicial tiene la capacidad de crear nuevos hilos a través de una llamada a una función de la biblioteca, como **`thread_create`**. Esta función suele tomar un parámetro que especifica el **procedimiento** que debe ejecutar el nuevo hilo. Lo importante aquí es que **no es necesario (ni posible) especificar un espacio de direcciones nuevo para el hilo**, ya que el nuevo hilo se ejecutará dentro del **espacio de direcciones** del hilo que lo creó. Esto significa que todos los hilos comparten el mismo espacio de direcciones.


Aunque en algunos casos los hilos pueden tener una **relación jerárquica** (padre e hijo), en muchos casos todos los hilos son **iguales** y no existe una estructura jerárquica. El hilo que crea a otros hilos (*el hilo creador*) generalmente recibe un **identificador único**, que es utilizado para identificar y referirse al nuevo hilo.


Cuando un hilo termina su trabajo, puede **terminar su ejecución** mediante una llamada como **`thread_exit`**. Después de hacer esto, el hilo desaparece y ya no puede ser **replanificado** para ejecutarse nuevamente. En sistemas con hilos, también existe la posibilidad de que un hilo **espere** a que otro hilo termine antes de continuar. Esto se logra a través de la llamada **`thread_join`**, que bloquea el hilo que la invoca hasta que el hilo especificado haya finalizado su ejecución.

Otra función importante es **`thread_yield`**, que permite a un hilo **ceder voluntariamente la CPU** para que otro hilo pueda ejecutarse. Dado que no hay un sistema de interrupciones de reloj como en la **multiprogramación de procesos**, es crucial que los hilos sean "amables" y cedan la CPU para que los demás hilos tengan la oportunidad de ejecutarse.

1. **Uso del sistema `fork` en UNIX**: Cuando un proceso con varios hilos invoca la llamada al sistema **`fork`**, puede ser necesario que el proceso hijo también herede todos los hilos del proceso padre. Esto podría generar **problemas** si un hilo del padre está bloqueado (por ejemplo, esperando una entrada desde el teclado) y el hijo también hereda ese bloqueo, lo que podría generar **inconsistencias** en el comportamiento del programa.

2. **Acceso a recursos compartidos**: Dado que los hilos comparten recursos como archivos abiertos, memoria dinámica, etc., pueden surgir **conflictos**. Por ejemplo:
   - Si un hilo cierra un archivo mientras otro hilo aún está leyendo de él, puede generarse un **error de acceso**.
   - Si un hilo detecta que hay poca memoria y empieza a asignar más memoria, y a la vez otro hilo también intenta hacer lo mismo, pueden producirse **duplicaciones** en la asignación de memoria.

Para resolver estos problemas, es necesario diseñar y gestionar cuidadosamente los **recursos compartidos** y las transiciones de los hilos para evitar **condiciones de carrera** y otros errores típicos en programas con múltiples hilos.

| **Llamada de hilo**          | **Descripción**                                   |
|------------------------------|---------------------------------------------------|
| `pthread_create`             | Crea un nuevo hilo                                |
| `pthread_exit`               | Termina el hilo llamador                          |
| `pthread_join`               | Espera a que un hilo específico termine          |
| `pthread_yield`              | Libera la CPU para dejar que otro hilo se ejecute|
| `pthread_attr_init`          | Crea e inicializa la estructura de atributos de un hilo |
| `pthread_attr_destroy`       | Elimina la estructura de atributos de un hilo    |


## Implementaciones
### En el espacio de usuario
El **kernel** no sabe nada sobre los hilos, que se implementan con una **biblioteca**. Esto permite hilos en sistemas operativos que no aceptan hilos, y requiere una **tabla de hilos privada por cada proceso**, **administrada en tiempo de ejecución por ese proceso**.

Cuando un hilo debe bloquearse, llama a un procedimiento en tiempo de ejecución, en el cual se **comprueba si el hilo debe bloquearse**. Si es necesario, se almacenan los **registros** del hilo **en la tabla de hilos**, **se busca otro hilo listo para ejecutarse** y se cargan los valores del nuevo hilo en los registros para comenzar su ejecución.

Como se puede ver, la conmutación entre hilos **es más rápida** que la conmutación entre procesos, ya que **no necesita un trap**, **vaciar la caché**, **cambiar el contexto**, etc. También permite el uso de algoritmos de planificación personalizados.

#### Ventajas
- **Algoritmos de planificación personalizados**.
- **Escalabilidad:** Una tabla de hilos central puede ser un problema cuando hay muchos hilos.

#### Problemas en este sistema:
- **Llamadas al sistema con bloqueo**: Pueden **parar todos los hilos** en un proceso. Esto se soluciona comprobando antes si habrá bloqueo.
- **Fallos de página**: Si un hilo tiene un fallo de página, **todos los hilos del proceso se bloquean**.

![[archivos/imagenes/Pasted image 20241117210811.png]]

### En el espacio de kernel
En este caso, es el **kernel** quien administra los hilos, por lo que no hay tabla de hilos en cada proceso ni se necesita un sistema en tiempo de ejecución**. El kernel almacena una tabla de hilos con el estado de cada uno**.

Para gestionarlos, se requieren **llamadas al sistema**. Cuando un hilo se bloquea, el kernel decide si se ejecuta otro hilo del mismo proceso o un hilo de otro proceso.
##### Ventajas:
- **Cuando el hilo se bloquea, el kernel puede ejecutar otro hilo del mismo proceso u otro hilo de otro porceso**.

##### Desventajas:
- **Velocidad**: Las llamadas al sistema son más costosas, lo que hace que el sistema sea más lento.
- **Problemas adicionales**: El SO debe resolver problemas como cuántos hilos tiene un proceso creado con `fork` o qué hilo atiende las señales.

### Implementaciones híbridas
Es un intento de **combinar las ventajas de los hilos en el espacio de usuario con las de los hilos en el espacio de núcleo**. En este enfoque, se implementa un sistema de hilos en el kernel que se **multiplexan** en varios hilos de usuario. 

El **programador** decide cuantos hilos de cada tipo se usan y el **SO** solo es consciente de los hilos en el espacio de **núcleo**

![[archivos/imagenes/Pasted image 20241117210852.png]]

### Hilos emergentes (Pop-up thread)
Los hilos emergentes se utilizan frecuentemente en sistemas distribuidos, que consisten en varias computadoras conectadas a través de una red, permitiendo que el usuario acceda a los recursos de todas las máquinas desde una sola.

El método tradicional es que un proceso esté bloqueado en una llamada al sistema `receive`, esperando un mensaje entrante. Cuando el mensaje llega, el proceso lo procesa y maneja. Sin embargo, es posible hacer que la llegada del mensaje cree un nuevo hilo para manejarlo. Este hilo que se crea se conoce como **hilo emergente**.

**Se crean con gran rapidez** porque no tienen historial (pila, registros...).

![[archivos/imagenes/Pasted image 20241117211021.png]]

### Hilos en Linux
En Linux, los hilos se gestionan de manera especial debido a su capacidad para combinar la distinción tradicional entre procesos e hilos. Aunque Linux es un sistema operativo **multitarea** que permite **ejecutar varios procesos de forma concurrente**, también es un sistema **multihilo** que permite ejecutar **múltiples hilos dentro de un mismo proceso**. A través de la llamada al sistema `clone`, Linux permite crear hilos con un control fino sobre lo que **se comparte entre hilos y lo que se mantiene privado.**

#### La llamada al sistema `clone`
La función `clone` en Linux permite la **creación de un nuevo hilo o proceso**, dependiendo de los parámetros que se pasen. Esta llamada es muy flexible porque, a diferencia de otros sistemas operativos donde los hilos se gestionan de forma independiente de los procesos, `clone` permite a los programadores decidir qué recursos o características se comparten entre el **hilo hijo y el hilo padre, y qué recursos se mantienen privados**.

La función `clone` tiene la siguiente firma:

```c
int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...);
```

**Parámetros de la función:**
1. **`fn`**: Es un puntero a una función que será ejecutada por el hilo recién creado. Cuando el hilo hijo es creado, empieza su ejecución en esta función.
  
2. **`child_stack`**: Es un puntero a la pila que se utilizará para el nuevo hilo. Este parámetro es necesario para la creación de un hilo, ya que cada hilo necesita su propia pila de ejecución.

3. **`flags`**: Es un parámetro de control que define cómo se compartirán los recursos entre el hilo padre y el hilo hijo. Aquí se especifican las propiedades que deben ser compartidas o privadas. Por ejemplo:
   - `CLONE_VM`: Comparte el espacio de direcciones de memoria (por lo tanto, los hilos comparten memoria).
   - `CLONE_FS`: Comparte la información del sistema de archivos.
   - `CLONE_FILES`: Comparte los descriptores de archivos.
   - `CLONE_SIGHAND`: Comparte las señales.
   - Y otros flags que permiten un control detallado sobre la creación del hilo.

4. **`arg`**: Es un argumento que se pasa a la función `fn` cuando se ejecuta. Puede ser cualquier dato que se desee pasar al hilo hijo al momento de su creación.

5. **Otros parámetros**: Dependiendo de la versión y de la plataforma, puede haber otros parámetros opcionales.

###### Características clave de `clone`:
- **Creación de hilos ligeros**: Con `clone`, se puede crear un hilo ligero (hilo en el sentido de ejecución concurrente) dentro de un proceso o crear un nuevo proceso. Si se desea que el hilo compartido utilice un espacio de memoria común, se puede usar la bandera `CLONE_VM`.
  
- **Flexibilidad en el manejo de recursos**: La principal ventaja de `clone` es que se pueden compartir o no ciertos recursos como memoria, descriptores de archivo, y más. Esto otorga gran flexibilidad para el manejo de la concurrencia en Linux.
  
- **Control sobre la pila**: Cada hilo tiene su propia pila de ejecución, lo que asegura que no haya colisiones entre los hilos en cuanto a los datos locales de cada uno. El parámetro `child_stack` especifica dónde debe estar ubicada la pila del hilo hijo.

# Planificación de Procesos
La **planificación** es una función esencial en sistemas operativos multiprogramados, donde múltiples procesos o hilos compiten por la CPU. El **planificador** decide qué proceso se ejecutará según un **algoritmo de planificación**, optimizando el uso del procesador y garantizando un rendimiento eficiente.

**La conmutación de procesos (cambio de contexto) es cara**:
1. Mediante un **trap** pasamos de modo usuario a modo **núcleo**
2. **Guardamos el estado del proceso y datos en la tabla de procesos y también el mapa de memoria**
3. **Se seleciona el nuevo proceso mediante el algoritmo de planificación**.
4. **Se carga el nuevo proceso**.

## Comportamiento de un Proceso
 Los procesos en **estado listo** **compiten por la CPU**. En PCs simples con pocos procesos, la planificación tiene menor impacto. En servidores, la planificación es crítica para aprovechar al máximo la CPU y minimizar el desperdicio de recursos.

![[archivos/imagenes/Pasted image 20241117222127.png]]

Los procesos pueden estar: 
   - **Limitados por CPU (cálculos como en la imagen a))**:
     - Ráfagas largas de cálculo con pocas esperas por E/S.
   - **Limitados por E/S (como la imagen b)**:
     - Ráfagas cortas de CPU con frecuentes operaciones de E/S.
El **factor clave** es la duración de la ráfaga de CPU. Procesos limitados por E/S necesitan obtener rápidamente la CPU para mantener otros dispositivos ocupados.

Vale la pena observar que, a medida que las CPUs se vuelven más rápidas, los procesos tienden a ser más **limitados a E/S**. La idea básica aquí es que, **si un proceso limitado a E/S desea ejecutarse, debe obtener rápidamente la oportunidad de hacerlo** para que pueda emitir su petición de disco y mantener el disco ocupado. Cuando los procesos **están limitados a E/S,** se requieren muchos de ellos para que la CPU pueda **estar completamente ocupada.**

![[archivos/imagenes/Pasted image 20241218085522.png]]

## Cuando Planificar Procesos
¿Debe ejecutarse el proceso **padre** o el **hijo** al crear un nuevo proceso? Porque ambos están en estado listo.

¿Qué proceso listo debe ejecutarse tras la **terminación** de otro?
Si no hay procesos listos, se ejecuta un proceso **inactivo**.

Si un proceso se **bloquea**, ¿se debe considerar la razón del bloqueo?
Ante una **interrupción de E/S**, ¿debe ejecutarse el proceso que estaba esperando?
Si ocurre una **interrupción de reloj**, ¿se cambia de proceso?

**Algoritmo de planificación no apropiativo**:
Selecciona un proceso y **lo ejecuta hasta que este se bloquee o termine**.
**Algoritmo de planificación apropiativo**:
Selecciona un proceso y lo ejecuta durante un **tiempo máximo fijo**. Si el proceso excede este tiempo, se suspende y se da paso a otro.

**Se busca:**
 **Minimizar cambios de contexto** para reducir el costo de las conmutaciones.
**Responder rápidamente a los eventos** (bloqueos, interrupciones) para garantizar un flujo de ejecución continuo.

## Metas de los Algoritmos de Planificación 
![[archivos/imagenes/Pasted image 20241217165513.png]]

## Planificación en Sistemas de Procesamiento por Lotes
Los sistemas de procesamiento por lotes suelen emplear algoritmos diseñados para maximizar la eficiencia del sistema, **minimizando el tiempo de retorno y optimizando el uso de recursos**. 
- No hay procesos que esperen para obtener una respuesta rápida.
- No apropiativos o apropiativos con largos períodos
- Reduce la conmutación de procesos

### FCFS, First-Come, First-Served
  Es el algoritmo más simple y **no apropiativo**.  
  Los procesos **se ejecutan en el orden en que llegan**, permaneciendo en ejecución hasta completarse.  
  Usa una cola única donde los procesos listos se ordenan por llegada.  

- **Ventajas**:  
  - Fácil de entender e implementar.  
  - Justo en el sentido de atender los procesos según su orden de llegada.  

- **Desventajas**:  
  - **Efecto convoy**: Procesos cortos pueden esperar mucho si están detrás de un proceso largo.  
  - Ineficiente para sistemas con una **mezcla de procesos intensivos en CPU y en E/S, pues los procesos de E/S pueden quedar inactivos demasiado tiempo**.  

---

### SJF, Shortest Job First**  
**No apropiativo**. 
Selecciona el trabajo con el **tiempo de ejecución más corto** entre los procesos listos para ejecutarse.  
Requiere que **se conozca de antemano la duración de los procesos**.  

- **Ventajas**:  
  - **Minimiza el tiempo promedio de respuesta** cuando todos los trabajos están disponibles al mismo tiempo.  
  - **Favorece a procesos cortos**, reduciendo tiempos de espera.  

- **Desventajas**:  
  - No es práctico si **no se pueden predecir los tiempos de ejecución**.  
  - Puede causar **injusticia** **hacia procesos largos** que esperan indefinidamente.  

- **Ejemplo**:  
  Consideremos 4 procesos con tiempos de ejecución de 8, 4, 4 y 4 minutos.  
  - En orden de llegada (A, B, C, D), los tiempos de respuesta promedio serían:  
    - A: 8, B: 12, C: 16, D: 20 → **Promedio: 14 minutos**.  
  - Con SJF (B, C, D, A):  
    - B: 4, C: 8, D: 12, A: 20 → **Promedio: 11 minutos**.  

![[archivos/imagenes/Pasted image 20241117222921.png]]

#### SRTN, Shortest Remaining Time Next
**Versión apropiativa del SJF.**  
Siempre selecciona el proceso con el **menor tiempo de ejecución restante.**  
Si llega un nuevo proceso con un tiempo de ejecución menor al restante del proceso actual, se realiza un **cambio de contexto para atender el nuevo proceso**.  

- **Ventajas**:  
  - Beneficia a trabajos cortos que llegan mientras otro proceso está en ejecución.  
  - Ofrece una buena respuesta promedio para sistemas dinámicos.  

- **Desventajas**:  
  - **Requiere conocer los tiempos de ejecución de antemano**.  
  - Mayor sobrecarga por los frecuentes cambios de contexto.  

## Planificación en Sistemas Interactivos
Estos algoritmos son comunes en computadoras personales, servidores y en otros tipos de sistemas también.
### Planificación por Turno Circular (Round-Robin)  
Cada proceso recibe un **tiempo fijo** **llamado *quántum*** para ejecutarse.  
Si el proceso **no termina dentro del quántum, se pausa y pasa al final de la cola,** permitiendo que otro proceso se ejecute.  
**Es apropiativo** y se implementa fácilmente con una lista circular de procesos.  

- **Ventajas**:  
  - Es **simple, equitativo y asegura que ningún proceso quede excluido**.  
  - Adecuado para sistemas interactivos, donde las tareas suelen ser cortas.  

- **Desventajas**:  
  - La **eficiencia depende del tamaño del quántum**:  
    - **Quántum corto**: **Demasiados cambios de contexto**, desperdiciando tiempo.  Por ejemplo si asignamos un cuantum de 4 ms y el cambio de contexto tarda 1ms
    - **Quántum largo**: **Tiempos de respuesta elevados** para tareas interactivas cortas.  100 ms de quantum podrían acumular muchos procesos en la cola.
  - **No considera la prioridad** o la naturaleza del proceso.  
**Tiempo óptimo de quantum 20-50 ms**

![[archivos/imagenes/Pasted image 20241117224134.png]]

### Planificación por Prioridad
 **Cada proceso recibe una prioridad**, y el sistema siempre ejecuta el **proceso con la prioridad más alta**.  
Puede ser **estática** (determinada al inicio) o **dinámica** (cambia durante la ejecución).  

- **Ventajas**:  
  - **Procesos críticos o urgentes obtienen la CPU más rápidamente**.  
  - Puede adaptarse a diferentes necesidades de un sistema multiusuario o multitarea.  

- **Desventajas**:  
  - **Inanición (Starvation)**: Procesos de **baja prioridad pueden quedar sin ejecutarse** si siempre hay procesos de alta prioridad.  
  - La **implementación de prioridades dinámicas puede ser compleja**.  

Para evitar la **inanición**, en cada interrupción de reloj se reduce la prioridad del proceso en ejecución.

Para **asignar la prioridad de forma dinámica**, hay que tener en cuenta que los procesos limitados a **E/S** se deben ejecutar de forma **inmediata**. 

$$Prioridad= \frac 1f$$
$f$ es la fracción del último quantum utilizada por el proceso.

Un proceso que sólo utilizó 1 mseg de su quántum de 50 mseg obtendría una prioridad de 50, mientras que un proceso que se ejecutara durante 25 mseg antes de bloquearse recibiría la prioridad 2 y un proceso que utilizara el quántum completo recibiría la prioridad 1.

#### Múltiples Colas
![[archivos/imagenes/Pasted image 20241217174229.png]]

Si hay procesos de prioridad 4, se seleccionan por turno circular dentro de esa clase.
Si la clase de prioridad 4 está vacía, se ejecutan los de la
clase de prioridad 3.
Las clases deben ajustarse dinámicamente

#### Proceso más corto a continuación

Ejecuta el proceso con el tiempo estimado de ejecución más corto basado en estimaciones de ejecuciones previas (usando promedios ponderados). Esto minimiza el tiempo de respuesta total.

**Estimación del tiempo de ejecución basada en ejecuciones anteriores:**

$$T = a \times T_0 + (1 - a) \times T_1$$

donde:
- $T$ es el tiempo estimado actual.
- $T_0$ es el tiempo previamente estimado.
- $T_1$ es el tiempo de la última ejecución.
- $a$ es un parámetro de ponderación tal que $0\leq a \leq 1$.


#### Planificación garantizada:
Cada usuario o proceso recibe una porción justa de la CPU proporcional al número de usuarios o procesos activos, asegurando equidad en la distribución de recursos.
 
$$T=\frac 1n$$

#### Planificación por sorteo:
Los procesos reciben boletos de lotería; se elige al azar un ganador para asignar recursos como tiempo de CPU. Más boletos implican mayor probabilidad de selección, permitiendo flexibilidad y equidad probabilística.

#### Planificación por partes equitativas:
Divide el tiempo de CPU entre usuarios en lugar de procesos. Esto garantiza que todos los usuarios obtengan su parte justa, independientemente del número de procesos que ejecuten.

## Planificación de Hilos
Cuando los sistemas manejan procesos con múltiples hilos, hay dos enfoques principales para su planificación, dependiendo de si los hilos están gestionados a nivel **usuario** o a nivel **kernel**. Cada método tiene sus características y limitaciones.

### A nivel usuario
Los hilos son gestionados completamente por el proceso, sin intervención del kernel.
El kernel selecciona un **proceso completo** para ejecutarlo y no sabe qué hilos existen dentro del proceso.
El planificador de hilos del proceso decide cuál hilo ejecutará dentro del tiempo asignado al proceso.
**Ventajas:**
  - **Rendimiento alto:** Cambiar de un hilo a otro dentro del proceso es rápido, ya que no implica un cambio de contexto a nivel del kernel.
  - **Flexibilidad:** El proceso puede implementar su propio algoritmo de planificación, adaptado a sus necesidades.

**Limitaciones:**
  - **Sin interrupciones por reloj:** Los hilos no pueden ser interrumpidos automáticamente, lo que puede llevar a que un hilo monopolice el tiempo del proceso.
  - **Bloqueo de E/S:** Si un hilo se bloquea esperando una operación de E/S, **todo el proceso se suspende**, ya que el kernel no distingue entre hilos.

### A nivel Kernel

El kernel gestiona cada hilo individualmente, tratándolos como unidades independientes.
El kernel asigna un quántum de tiempo a cada hilo, sin importar a qué proceso pertenezca.
Si un hilo se bloquea, otros hilos (incluso del mismo proceso) pueden seguir ejecutándose.

**Ventajas:**
  - **Mejor manejo de bloqueos:** Si un hilo espera E/S, otros hilos del mismo proceso pueden continuar ejecutándose.
  - **Planificación más general:** El kernel puede gestionar los hilos de todos los procesos con un enfoque unificado.

**Limitaciones:**
  - **Rendimiento más bajo:** Cambiar entre hilos a nivel kernel requiere una conmutación de contexto completa, lo que es más lento debido a cambios en el mapa de memoria y la invalidación de la caché.
**Menos optimización específica:** El kernel no tiene conocimiento detallado de la lógica de la aplicación, lo que limita la capacidad de optimizar la planificación de hilos según las necesidades específicas de un programa.

![[archivos/imagenes/Pasted image 20241217190034.png]]
