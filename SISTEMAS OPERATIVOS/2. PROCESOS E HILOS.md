[[archivos/bibliografia/2-Sistemas operativos moderno 3ed Tanenbaum.pdf|2-Sistemas operativos moderno 3ed Tanenbaum]]

Escrito por **Adrián Quiroga Linares**.

El proceso es el concepto central en el diseño de cualquier sistema operativo, representando una abstracción de un programa en ejecución. Es crucial comprender este concepto, ya que todo lo demás en un sistema operativo depende de él.

Los procesos son una de las abstracciones más antiguas e importantes que ofrecen los sistemas operativos. Permiten la ejecución concurrente (aparente) de programas, incluso con una sola CPU, simulando múltiples CPU virtuales. Sin esta abstracción, la computación moderna sería inviable.

En este capítulo se analiza a fondo el concepto de procesos y su relación con una extensión más moderna: los **hilos (threads)**.

# Procesos
En las computadoras modernas, la **concurrencia** es esencial para manejar múltiples tareas simultáneamente, como en un servidor web que atiende múltiples solicitudes o en una PC personal con procesos en segundo plano (*correo, antivirus*) y actividades del usuario. Los **procesos** son clave para modelar y gestionar esta concurrencia.

En sistemas de **multiprogramación**, la CPU alterna rápidamente entre procesos, ejecutando cada uno durante breves periodos. Esto crea la ilusión de **paralelismo** (o **pseudoparalelismo**) en sistemas con una sola CPU. En contraste, los sistemas **multiprocesadores** logran **paralelismo real** mediante varias CPUs que comparten memoria física.

Para simplificar el manejo del paralelismo, los sistemas operativos han adoptado un modelo basado en **procesos secuenciales**, facilitando el diseño y administración de múltiples tareas concurrentes. Este modelo y sus implicaciones son el enfoque principal del capítulo.

### El modelo del proceso
#### **1. ¿Qué es un proceso?**
Un proceso es una **instancia de un programa en ejecución**, incluyendo:
- **Contador de programa lógico:** Indica la próxima instrucción a ejecutar.
- **Registros y variables:** Contienen los datos que el proceso necesita.
- **Estado:** Describe si el proceso está en **ejecución, esperando, o detenido**.

Aunque físicamente solo una CPU real ejecuta instrucciones a la vez, el sistema operativo con **multiprogramación** alterna entre procesos de manera rápida. Esto crea la **ilusión de ejecución paralela** o "pseudoparalelismo". 

#### **2. Multiprogramación y conmutación de procesos**
En un sistema de multiprogramación:
1. La CPU **ejecuta** un proceso durante un breve período.
2. **Guarda el estado** del proceso actual (incluyendo el contador de programa).
3. **Carga el estado de otro proceso y continúa su ejecución**.

El cambio entre procesos permite **aprovechar los tiempos muertos**, como cuando un proceso espera una operación de entrada/salida. En un período suficiente, todos los procesos progresan, pero **en cualquier instante solo uno está activo**.

![[archivos/imagenes/Pasted image 20241117162520.png]]

#### **3. Diferencia entre programa y proceso**
La diferencia clave es que:
- **Programa:** Es **estático**, un conjunto de instrucciones almacenadas (por ejemplo, una receta de cocina).
- **Proceso:** Es **dinámico**, la actividad de ejecutar un programa con entradas y salidas específicas (como seguir la receta para hornear un pastel).

**Ejemplo:**  
Un científico sigue una receta para hacer un pastel (proceso 1). Si ocurre una emergencia (su hijo es picado por una abeja), el científico guarda su progreso, atiende al niño siguiendo un libro de primeros auxilios (proceso 2), y luego vuelve al pastel. Este ejemplo muestra:
- **Conmutación de procesos:** La CPU cambia de un proceso a otro según prioridades.
- **Estado guardado y restaurado:** Permite continuar desde donde se dejó.

#### **4. Importancia de los procesos**
El modelo del proceso simplifica la gestión de tareas concurrentes en un sistema operativo, permitiendo:
- **Ejecutar múltiples aplicaciones simultáneamente**.
- **Compartir recursos de manera eficiente**.
- **Gestionar prioridades y garantizar que los procesos importantes reciban atención**.

En resumen, un **proceso** es la actividad de ejecutar un programa con un estado y recursos propios. Gracias al modelo de procesos, los sistemas operativos modernos pueden manejar múltiples tareas a la vez, dando la impresión de paralelismo incluso en sistemas con una sola CPU.

### Creación de un proceso
La **creación de un proceso** es un paso esencial en cualquier sistema operativo, ya que permite la ejecución de programas. Dependiendo del tipo de sistema, los procesos pueden ser creados de diversas maneras. En sistemas simples o de propósito específico, los procesos se crean al inicio, mientras que en sistemas más complejos, como los sistemas operativos de propósito general, los procesos pueden crearse y terminarse en cualquier momento durante su ejecución.

#### **Eventos que provocan la creación de un proceso:**
1. **Arranque del sistema**: Cuando el sistema operativo arranca, se crean varios procesos, algunos para tareas en **primer plano** (*interactúan con el usuario*) y otros en **segundo plano** (*funciones del sistema como gestión de correo electrónico o páginas web*). Estos últimos se conocen como **demonios** y trabajan en segundo plano desde el arranque del ordenador hasta apagarlo. Para listar los procesos que se están ejecutando usamos `ps`.
   
2. **Ejecutar una llamada al sistema desde un proceso existente**: Un proceso en ejecución puede **crear nuevos procesos para ayudarlo a realizar una tarea más eficiente**. Por ejemplo, si se necesita obtener grandes cantidades de datos, un proceso puede crear un nuevo proceso para descargar los datos mientras otro los procesa.

3. **Petición del usuario**: Los usuarios pueden crear procesos al **ejecutar programas desde la interfaz**, ya sea mediante un comando o haciendo clic en un ícono. **Esto generalmente lanza un nuevo proceso** que se ejecuta en su propia ventana.

4. **Inicio de un trabajo por lotes**: En sistemas de procesamiento por lotes, como en mainframes, los trabajos pueden ser enviados en cola y, cuando el sistema tiene recursos disponibles, se crea un proceso para ejecutar el siguiente trabajo de la cola.

#### **Detalles técnicos de la creación de un proceso:**
La creación de un proceso generalmente ocurre a través de una **llamada al sistema** que indica al sistema operativo que debe crear un nuevo proceso. La forma en que esto se implementa varía según el sistema operativo.

- **En UNIX**: Se utiliza una llamada al sistema llamada **`fork`**. Esta función crea un proceso hijo, que es un clon exacto del proceso que hizo la llamada. Luego, el proceso hijo puede utilizar **`execve`** (*o una llamada similar*) para reemplazar su imagen de memoria y ejecutar un nuevo programa. Esto permite que el proceso hijo realice una tarea diferente de su proceso padre.
  
  - El proceso **padre** y el **proceso hijo** comparten algunos recursos como **archivos abiertos**, pero cada uno tiene su **propio** **espacio de direcciones**, lo que significa que **no comparten memoria modificable**. El proceso hijo comienza con una copia del espacio de direcciones del padre.

#### **Resumen:**
La creación de un proceso es un mecanismo clave para que los sistemas operativos gestionen la ejecución de programas. Dependiendo del sistema operativo, el proceso puede ser creado **al inicio del sistema, por una acción del usuario, o de manera automática por el propio sistema operativo**. Una vez creado, el proceso tiene su propio espacio de direcciones y puede compartir algunos recursos con su proceso padre, pero generalmente no comparte memoria modificable.

> [!IMPORTANTE]
> Se realiza mediante la llamada al sistema fork, que crea un proceso hijo igual al proceso padre (en imagen de memoria, variables de retorno, archivos abiertos...). Pero se ubica en una región de memoria diferente.
> 
> `pid_t fork(void)` 

### Terminación de procesos
La **terminación de un proceso** es el paso final de su ciclo de vida. Los procesos concluyen por diversas razones, y su finalización puede ser **voluntaria** o **involuntaria**, dependiendo de las circunstancias que enfrenten. Las principales causas de terminación de procesos son las siguientes:

#### **1. Salida normal (voluntaria):**
La mayoría de los procesos terminan porque **completan el trabajo que se les asignó**. Por ejemplo:
- Un compilador finaliza después de compilar un programa y llama al sistema operativo para señalar su conclusión (*mediante la llamada al sistema `exit` en UNIX*).
- En aplicaciones gráficas como procesadores de texto o navegadores, el usuario puede optar por **cerrar el programa haciendo clic en un botón** o seleccionando una opción de menú. Esto provoca que el proceso elimine archivos temporales y luego termine.

> [!Importante]
> El proceso sabe que va a morir
> `void _exit(int status)`

#### **2. Salida por error (voluntaria):**
Un proceso puede decidir terminar cuando **detecta un error que le impide continuar**, como:
- Intentar abrir un archivo que no existe (por ejemplo, al compilar un archivo no encontrado como `gcc foo.c`).
- Aunque algunos programas, especialmente los interactivos, manejan los errores pidiendo al usuario que corrija la entrada (*por ejemplo, mostrando un cuadro de diálogo*), otros simplemente se detienen.

#### **3. Error fatal (involuntaria):**
Los errores fatales suelen ser causados por fallas graves en el programa. Ejemplos incluyen:
- **Ejecutar una instrucción ilegal**, como intentar usar una operación no soportada por la CPU.
- **Acceso no válido a memoria**, como intentar leer o escribir en una dirección fuera del espacio asignado.
- **División entre cero**, que no tiene un resultado definido.
  
En algunos sistemas, como UNIX, los procesos pueden registrarse para manejar ciertos errores (*por ejemplo, señales como `SIGSEGV` para fallos de segmentación*). En lugar de terminar de inmediato, el sistema operativo envía una **señal** al proceso para que intente gestionar el error antes de detenerse.

#### **4. Eliminación por otro proceso (involuntaria):**
Un proceso puede ser terminado por otro proceso mediante una llamada al sistema. Esto puede suceder si:
- El proceso eliminador decide que otro proceso debe finalizar (por ejemplo, porque está consumiendo demasiados recursos o está bloqueado).
- **En UNIX, la llamada al sistema utilizada es** **`kill`**.
  
En ambos casos, el proceso que desea terminar otro debe tener los permisos necesarios para hacerlo. Por ejemplo, un proceso con privilegios normales no puede finalizar un proceso del sistema.

> [!Importante]
> El proceso no sabe que a morir.
> `int kill(pid_t pid, int sig);` si no se especifica la señal, manda un SIGTERM (*que mata al proceso*)
****
#### **Efectos de la terminación de un proceso:**
- En algunos sistemas operativos, cuando un proceso termina, todos los procesos creados por él también se eliminan. Sin embargo, ni **Windows** ni **UNIX** adoptan este comportamiento de manera predeterminada.
- La terminación de un proceso incluye la **liberación de los recursos que estaba** utilizando, como memoria, descriptores de archivo y entradas en las tablas del sistema operativo.

### **Esperar a la terminación de un proceso**
Cuando un proceso crea un hijo mediante una llamada como `fork`, es habitual que el proceso padre necesite esperar a que el hijo termine antes de continuar con su ejecución. Esto se logra a través de las funciones **`wait`** y **`waitpid`**.


> [!Importante]
> **Tipos de procesos:**
> - **Proceso Zombi:** proceso que **ya ha terminado su tarea**, es decir, está muerto, pero no ha sido eliminado del sistema, es decir, sigue en la tabla de procesos. Esto se debe a que los procesos solo se borran de la tabla cuando su padre finaliza o hasta que ejecute un **wait**o **waitpid** asociado a él. Todos los procesos al acabar pasan a ser **zombies** hasta que se realice una de las dos acciones que provocan su eliminación de la tabla de procesos
> - **Proceso huérfano:** es posible que un proceso padre cree a un hijo y **finalice antes el padre que el hijo**, creando así un hijo **huérfano**. Cuando un huérfano finaliza, **no se convierte en zombi** porque ya no tiene un padre que esté esperando por él, a pesar de haber sido adoptado, por que lo una ver terminado **se elimina directamente de la tabla de procesos**. Los procesos huérfanos son adoptados por el **proceso init**


#### **Uso de `wait`**
- **Firma:**
  ```c
  pid_t wait(int *status);
  ```
- **Parámetros:**
  - `int *status`: Dirección de memoria donde se almacenará el estado de finalización del proceso hijo.
    - Este estado incluye información como:
      - Si el proceso terminó normalmente o debido a una señal.
      - El valor de retorno del hijo (si terminó normalmente).

- **Retorno:**
  - Devuelve el **PID** del hijo que terminó.
  - Retorna **-1** si ocurre un error, por ejemplo:
    - El proceso no tiene hijos activos.
    - Una señal interrumpe la llamada.

- **Comportamiento:**
  - **El proceso padre se bloquea** (queda en espera) hasta que cualquiera de sus procesos hijos finalice.
  - Útil para gestionar procesos de manera general, sin necesidad de identificar cuál hijo específico ha terminado.

#### **Uso de `waitpid`**
- **Firma:**
  ```c
  pid_t waitpid(pid_t pid, int *stat_loc, int options);
  ```
- **Parámetros:**
  - `pid`: Identifica al proceso hijo específico que se está esperando:
    - **-1:** Espera a cualquier hijo (similar a `wait`).
    - **PID específico:** Espera por ese hijo en particular.
  - `int *stat_loc`: Igual que en `wait`, almacena el estado del proceso hijo.
  - `options`: Opciones adicionales que controlan el comportamiento de la función, como:
    - `WNOHANG`: No bloquea al padre; si no hay procesos que hayan terminado, regresa de inmediato.

- **Retorno:**
  - Devuelve el PID del hijo esperado que terminó.
  - Retorna **0** si no hay procesos finalizados y se utiliza `WNOHANG`.
  - Retorna **-1** en caso de error.

- **Ventajas sobre `wait`:**
  - Permite mayor control al especificar qué hijo esperar.
  - Permite trabajar en modo no bloqueante (útil en sistemas concurrentes).

#### **Estado de finalización del hijo (`status`):**
El contenido del parámetro `status` puede ser interpretado mediante macros como:
- **`WIFEXITED(status)`**: Devuelve verdadero si el hijo terminó normalmente.
- **`WEXITSTATUS(status)`**: Recupera el código de salida del hijo si terminó normalmente.
- **`WIFSIGNALED(status)`**: Devuelve verdadero si el hijo terminó debido a una señal.
- **`WTERMSIG(status)`**: Recupera la señal que terminó al hijo.

### **Invocación de otros programas**
Cuando un proceso necesita reemplazar su contenido actual (código, datos, pila) con el de un programa diferente, utiliza funciones de la familia `exec`. Una de las más generales es **`execve`**.

#### **Uso de `execve`**
- **Firma:**
  ```c
  int execve(const char *path, char *const argv[], char *const envp[]);
  ```
- **Parámetros:**
  - `const char *path`: Ruta al archivo ejecutable que se desea cargar.
  - `char *const argv[]`: Arreglo de cadenas que representa los argumentos del programa.
    - El primer argumento suele ser el nombre del programa.
    - Equivalente a los argumentos del `main(argc, argv)` en C.
  - `char *const envp[]`: Arreglo de cadenas que representan el entorno del programa.
    - Equivalente a `envp` en C, contiene variables de entorno.

- **Funcionamiento:**
  - Reemplaza completamente el contenido del proceso actual con el del programa especificado.
  - No crea un nuevo proceso; el proceso actual mantiene su PID.
  - Si tiene éxito, **no regresa**, ya que el proceso anterior deja de existir.
  - Si falla (por ejemplo, el archivo no existe o no tiene permisos), retorna **-1**.

- **Casos de uso:**
  - Un shell típico usa `fork` para crear un proceso hijo y luego `execve` en el hijo para ejecutar el comando que el usuario solicita.

#### **Relación con el `main`:**
El prototipo de `execve` se asemeja a la forma del `main` en C:
```c
int main(int argc, char *argv[], char *envp[]);
```
- Los argumentos (`argv`) y el entorno (`envp`) del nuevo programa se pasan directamente a `execve`.

> **Ejemplo típico:**
```c
#include <unistd.h>
#include <stdio.h>

int main() {
    char *argv[] = { "/bin/ls", "-l", NULL };
    char *envp[] = { NULL };
    
    if (execve("/bin/ls", argv, envp) == -1) {
        perror("execve failed");
    }
    return 0;
}
```
En este ejemplo:
- Se ejecuta el comando `ls -l`.
- El proceso actual se reemplaza por el programa `ls`.
- Si `execve` falla, imprime un mensaje de error.

Esto viene siendo lo mismo que **cambiar la imagen del proceso**

Un **proceso** se compone de varias regiones en memoria:

- **Código**: Instrucciones que el proceso ejecuta.
- **Datos**: Variables y estructuras de datos que utiliza.
- **Pila (stack)**: Espacio para llamadas a funciones, parámetros y variables locales.
- **Segmento de entorno**: Variables de entorno y configuración del sistema.

Cuando cambiamos la imagen de un proceso, **se reemplazan todas estas regiones con las correspondientes al programa nuevo**, manteniendo el mismo **PID**. Es como si el proceso existente se "transformara" en otro programa.

En sistemas basados en UNIX, esto se logra con las funciones de la familia `exec`, como `execve`, `execl`, `execvp`, etc. Estas funciones:

1. Descargan la imagen actual del proceso (su código, datos, pila y demás).
2. Cargan en su lugar la imagen del programa especificado.
3. Inician la ejecución del nuevo programa desde su punto de entrada (generalmente la función `main`).

Cambiar la imagen de un proceso es diferente de crear uno nuevo porque:

- **No se genera un nuevo PID**: El proceso original sigue existiendo, pero con un contenido completamente distinto.
- **No se heredan las características del programa anterior**: Toda la memoria (código, datos, pila) se sobrescribe, excepto algunos atributos del sistema operativo como el PID o los descriptores de archivo abiertos.

**Utilidades:**
1. **Eficiencia**: En lugar de cerrar un proceso y abrir otro, se reutiliza el proceso existente.
2. **Control**: Permite que un programa, como un shell, controle la ejecución del nuevo programa (por ejemplo, esperar a que termine o manejar su salida).
3. **Simplicidad del diseño**: Un sistema operativo no necesita gestionar el cambio de contexto entre dos procesos diferentes.

### Señales
Las **señales** son un mecanismo de comunicación **asíncrono** utilizado en los sistemas operativos basados en POSIX para notificar a un proceso de eventos del sistema o para sincronización entre procesos. Son esenciales para manejar interrupciones, errores y acciones específicas del sistema operativo o de otros procesos.

#### **Características principales de las señales**
1. **Identificación:**
   - Las señales se identifican mediante **enteros** (números) que representan eventos específicos.
   - También se utilizan **constantes simbólicas** (ej., `SIGKILL`, `SIGTERM`) para facilitar su uso.

2. **Finalidad:**
   - **Notificar eventos:** Señalar que ocurrió algo relevante, como un temporizador que expiró.
   - **Control de procesos:** Finalizar, suspender, o reanudar un proceso.
   - **Sincronización:** Coordinar procesos, como en el caso de señales definidas por el usuario (`SIGUSR1`, `SIGUSR2`).

3. **Comportamiento:**
   - Para la mayoría de las señales, se puede definir un manejador específico (custom signal handler) que ejecute acciones personalizadas.
   - Algunas señales (como `SIGKILL`) no se pueden ignorar ni manejar de forma personalizada.

#### **Señales comunes en POSIX**
| Señal      | Descripción                                                                 |
|------------|-----------------------------------------------------------------------------|
| **SIGABRT**| Solicita que el proceso aborte y genere un núcleo de depuración (*core dump*).|
| **SIGALRM**| Indica que expiró un temporizador configurado por el proceso.               |
| **SIGFPE** | Error matemático (como división por cero o error de punto flotante).        |
| **SIGHUP** | Indica que se desconectó la terminal o línea telefónica asociada.           |
| **SIGILL** | Instrucción ilegal detectada en el proceso.                                 |
| **SIGKILL**| Termina el proceso inmediatamente (no se puede capturar ni ignorar).       |
| **SIGPIPE**| Escritura en un *pipe* sin lectores activos.                                |
| **SIGSEGV**| Violación de memoria (acceso a una dirección no válida).                   |
| **SIGTERM**| Solicita la finalización ordenada del proceso.                             |
| **SIGUSR1**| Señal personalizada definida por el usuario.                               |
| **SIGUSR2**| Otra señal personalizada para propósitos definidos por el usuario.         |

#### **Generación de señales**
Las señales pueden generarse por diferentes causas:
1. **Acciones del usuario:**
   - Pulsar teclas específicas como `Ctrl+C` (`SIGINT`) o `Ctrl+\` (`SIGQUIT`).
2. **Eventos del sistema:**
   - Temporizadores (`SIGALRM`), acceso inválido a memoria (`SIGSEGV`), etc.
3. **Otros procesos:**
   - Usando la llamada al sistema `kill()` para enviar una señal a un proceso.

```c
int kill(pid_t pid, int sig);
```
- **`pid`**: Identifica el proceso al que se envía la señal.
- **`sig`**: Especifica la señal a enviar.

#### **Manejo de señales**
El sistema operativo proporciona funciones para manejar señales:
1. **Definir un manejador con `signal`:**
   Permite asignar un manejador (*función*) personalizado para una señal.

   ```c
   sighandler_t signal(int signum, sighandler_t handler);
   ```
   - **`signum`**: Señal a manejar (ej., `SIGTERM`).
   - **`handler`**: Acción asociada (puede ser:
     - `SIG_DFL`: Acción predeterminada.
     - `SIG_IGN`: Ignorar la señal.
     - Una función definida por el usuario).

2. **Uso avanzado con `sigaction`:**
   Ofrece mayor control sobre las acciones de las señales.

   ```c
   int sigaction(int signum, const struct sigaction *act, struct sigaction *oldact);
   ```
   - **`act`**: Define la nueva acción.
   - **`oldact`**: Guarda la acción anterior.

3. **Más funciones relacionadas con señales:**
   - **`sigprocmask`**: Modifica o consulta la máscara de señales bloqueadas.
   - **`sigpending`**: Examina las señales pendientes.
   - **`sigsuspend`**: Suspende el proceso hasta recibir una señal específica.


### Resumen de lo aprendido

1. **`sigemptyset`**:
   - Limpia (vacía) un conjunto de señales, dejándolo sin señales.

2. **`sigaddset`**:
   - Agrega una señal específica a un conjunto de señales.

3. **`sigprocmask`**:
   - Controla la máscara de señales bloqueadas del proceso actual:
     - Puede bloquear, desbloquear o reemplazar las señales de un conjunto.

4. **`sigpending`**:
   - Obtiene las señales pendientes (que están bloqueadas y no han sido entregadas aún).

5. **`sigismember`**:
   - Comprueba si una señal específica pertenece a un conjunto de señales.

6. **`WIFEXITED`**:
   - Determina si un proceso hijo terminó normalmente (mediante `exit` o regreso del `main`).

### Ejemplo práctico:
Supongamos que estás manejando señales en un programa:

- **Crear y manipular conjuntos de señales**:
   - Usar `sigemptyset` y `sigaddset` para definir qué señales bloquear.
   - Usar `sigprocmask` para bloquear o desbloquear esas señales.

- **Consultar el estado de señales**:
   - Usar `sigpending` para ver qué señales están en espera.
   - Usar `sigismember` para comprobar si una señal específica está bloqueada.

- **Esperar y gestionar procesos**:
   - Usar `WIFEXITED` al esperar un proceso hijo con `wait` o `waitpid` para saber si terminó correctamente.

Este conocimiento es fundamental para manejar señales, coordinar procesos y garantizar un comportamiento controlado en programas complejos.

### **Ejemplo: Manejador de señal personalizada**

```c
#include <stdio.h>
#include <signal.h>
#include <unistd.h>

void signal_handler(int signum) {
    printf("Señal %d recibida\n", signum);
}

int main() {
    signal(SIGUSR1, signal_handler);  // Asignar manejador a SIGUSR1

    printf("Esperando señales...\n");
    while (1) {
        pause();  // Suspender hasta recibir una señal
    }

    return 0;
}
```
- Este programa espera señales indefinidamente y maneja `SIGUSR1` imprimiendo un mensaje.

---

### **Otros mecanismos asociados**
1. **Alarma (`alarm`):**
   Configura un temporizador que envía `SIGALRM` cuando expira.

   ```c
   unsigned int alarm(unsigned int seconds);
   ```
   Ejemplo: `alarm(5)` enviará `SIGALRM` al proceso después de 5 segundos.

2. **Suspender con `pause`:**
   Detiene el proceso hasta que reciba una señal.

   ```c
   int pause(void);
   ```

---

### **Resumen**
- **Señales** son herramientas potentes para la comunicación y sincronización asíncrona en sistemas POSIX.
- **Envío:** Con `kill`.
- **Manejo:** Con `signal`, `sigaction`, y otros.
- **Usos:** Finalización de procesos, temporizadores, manejo de errores y comunicación entre procesos.

### **Jerarquías de procesos**
#### **¿Qué es una jerarquía de procesos?**
Una **jerarquía de procesos** se da cuando un proceso (padre) crea otros procesos (hijos), y estos, a su vez, pueden crear sus propios descendientes. Esto genera una estructura similar a un árbol. Aunque un proceso puede tener múltiples hijos, **siempre tiene un único padre**. 

La relación padre-hijo puede implicar ciertas dependencias o formas de asociación, que varían según el sistema operativo.

#### **Jerarquías en UNIX:**
- En UNIX, un proceso y sus hijos forman un **grupo de procesos**. 
  - Ejemplo: Si un usuario trabaja en una terminal o ventana y envía una señal (como una interrupción con `Ctrl+C`), esta afecta a todos los procesos del grupo asociado.
- Los procesos en UNIX están organizados en una estructura jerárquica:
  1. **Proceso raíz:** Es el proceso inicial llamado **`init`** (o su equivalente en sistemas modernos como `systemd`), que se inicia al encender el sistema.
  2. **Hijos de `init`:** `init` lee un archivo de configuración para identificar las terminales conectadas y crea un proceso para cada una. Cada terminal espera inicios de sesión.
  3. **Cascada:** Cuando un usuario inicia sesión, se ejecuta un **shell**, que puede lanzar nuevos procesos, formando un árbol donde **`init`** es la raíz.

> Esta jerarquía facilita la administración, ya que cada proceso puede rastrearse a un ancestro común.


### **Estados de un proceso**
#### **¿Qué son los estados de un proceso?**
Un **proceso** pasa por diferentes estados durante su ciclo de vida dependiendo de las tareas que realiza y de los recursos disponibles en el sistema. Los tres estados principales son:

1. **Ejecución:**
   - El proceso está utilizando la CPU activamente en ese momento.
2. **Listo:**
   - El proceso está preparado para ejecutarse, pero espera su turno porque la CPU está ocupada con otro proceso.
3. **Bloqueado:**
   - El proceso no puede continuar porque necesita que ocurra un evento externo (por ejemplo, la llegada de datos de entrada).

![[archivos/imagenes/Pasted image 20241209105213.png]]

> [!Info] **Suspendido-> en memoria secundaria**
> Motivos para la suspension de un proceso:
![[archivos/imagenes/Pasted image 20241209105515.png]]

#### **Transiciones entre estados:**
El movimiento de un proceso entre estos estados se describe mediante un **diagrama de estados**, donde se identifican cuatro transiciones principales:

1. **De ejecución a bloqueado:**
   - Ocurre cuando el proceso necesita esperar un evento, como datos de entrada. Por ejemplo:
     - Leer desde un archivo que aún no está disponible.
     - Esperar la respuesta de una red.
   - En UNIX, esto puede suceder automáticamente cuando un proceso intenta leer datos y no hay nada disponible.

2. **De ejecución a listo:**
   - El planificador decide que el proceso ha usado suficiente tiempo de CPU y lo sustituye por otro proceso. Este cambio ocurre sin que el proceso lo perciba.

3. **De listo a ejecución:**
   - El planificador selecciona el proceso para que utilice la CPU nuevamente porque ha llegado su turno.

4. **De bloqueado a listo:**
   - Ocurre cuando el evento externo que esperaba el proceso (por ejemplo, la llegada de datos) se ha producido. Si no hay otro proceso en ejecución, este puede pasar inmediatamente a ejecución; de lo contrario, debe esperar en la cola de procesos listos.

![[archivos/imagenes/Pasted image 20241117175251.png]]


#### **Relación con el planificador:**
El **planificador de procesos** es una parte fundamental del sistema operativo que decide qué proceso debe ejecutarse y cuándo. Su objetivo es balancear:
- **Eficiencia:** Aprovechar al máximo la CPU.
- **Equidad:** Garantizar que todos los procesos tengan oportunidades razonables de ejecutarse.

> Un proceso puede moverse varias veces entre estos estados durante su vida útil. Por ejemplo:
> - Un proceso de usuario esperando entrada del teclado estará bloqueado hasta que el usuario escriba algo.
> - Cuando la entrada esté lista, pasará al estado listo y eventualmente será seleccionado para ejecución.



#### **Ejemplo práctico de interacción:**
En un sistema UNIX, el comando:

```bash
cat capitulo1 capitulo2 capitulo3 | grep arbol
```

1. **`cat`:** Concatenará los contenidos de los archivos y enviará la salida al siguiente proceso.
2. **`grep`:** Filtrará las líneas que contengan la palabra "arbol".

Si `grep` está listo para procesar pero `cat` aún no ha producido datos, **`grep` se bloqueará** hasta que reciba la entrada necesaria.

#### **Modelo de manejo de procesos:**
El sistema operativo puede representarse como un conjunto de procesos que gestionan diferentes tareas:
- **Procesos de usuario:** **Ejecutan comandos y aplicaciones**.
- **Procesos del sistema:** **Administran tareas como entrada/salida de disco o servicios de red**.

Cuando ocurre una interrupción (por ejemplo, el disco termina de leer un archivo), el sistema desbloquea el proceso correspondiente para que continúe ejecutándose. Este modelo simplifica la comprensión de cómo el sistema operativo organiza y administra el trabajo.


### Tabla de procesos
![[archivos/imagenes/Pasted image 20241117183104.png]]


La **tabla de procesos** es una estructura de datos mantenida por el sistema operativo, que almacena información sobre todos los procesos en ejecución o en espera en el sistema. Esta tabla es fundamental para la gestión de procesos, ya que organiza y guarda datos necesarios para el seguimiento, la programación y la ejecución de los procesos.

#### **Estructuras principales relacionadas con la tabla de procesos**
##### 1. **Pila del kernel**
   - La **pila del kernel** es una región de memoria exclusiva para cada proceso que se utiliza cuando este entra en **modo kernel** (*modo privilegiado del sistema operativo*).
   - **Propósito:**
     - Almacenar las funciones o rutinas que invoca el sistema operativo en nombre del proceso.
     - Guardar el estado actual del procesador, como registros, contexto de ejecución y direcciones de retorno.
   - **Uso:** Es especialmente importante durante llamadas al sistema, interrupciones o excepciones.

##### 2. **Área U (User Area o Área de Usuario)**
   - Aunque el nombre sugiere una relación con el espacio de usuario, el **área U** reside en el espacio de memoria del kernel.
   - **Contenido:** 
     - Contiene información de control específica de cada proceso que el kernel necesita mientras ejecuta dicho proceso.
     - Incluye datos como:
       - Tabla de descriptores de archivos abiertos.
       - Estado de señales y manejadores.
       - Información de credenciales del proceso (ID de usuario, ID de grupo, etc.).
       - Información de contabilidad (uso de CPU, tiempos de ejecución, etc.).
   - **Propósito:** Facilitar el acceso rápido a los datos críticos del proceso.

##### 3. **Tabla de regiones**
   - La **tabla de regiones** almacena información sobre las regiones de memoria asignadas a un proceso.
   - **Región de memoria:** Es un segmento lógico de memoria que el kernel administra. Puede incluir:
     - Región de código (instrucciones ejecutables del programa).
     - Región de datos (variables globales, estructuras).
     - Pila de usuario (para funciones y variables locales).
     - Mapeos específicos como memoria compartida.
   - **Entradas en la tabla:**
     - Dirección base de la región.
     - Tamaño de la región.
     - Permisos (lectura, escritura, ejecución).
     - Estado (por ejemplo, si está compartida o privada).

---

#### **Interacción de estas estructuras**
Cuando un proceso está activo, el sistema operativo utiliza estas estructuras de la siguiente manera:

1. **Modo usuario:**
   - El proceso opera principalmente en su propio espacio de memoria, utilizando la pila de usuario y otras estructuras como variables locales.

2. **Cambio a modo kernel:**
   - Si ocurre una interrupción, una excepción o una llamada al sistema:
     - El proceso cambia a **modo kernel**.
     - Los detalles del estado actual del proceso se almacenan en la pila del kernel.
     - El kernel utiliza la información en el área U para gestionar la solicitud del proceso.

3. **Gestión de memoria:**
   - Las regiones de memoria activas del proceso se identifican y gestionan usando la tabla de regiones.
   - Esto incluye operaciones como asignación dinámica, memoria compartida y protección de acceso.

#### **Ejemplo Práctico**
Supongamos que un proceso realiza una llamada al sistema para leer datos de un archivo. El flujo sería:

1. **En modo usuario:**
   - El proceso prepara los parámetros para la llamada al sistema (nombre del archivo, buffer, tamaño, etc.) y ejecuta la instrucción.

2. **Transición a modo kernel:**
   - El sistema operativo guarda el estado del proceso en la pila del kernel.
   - Usa el área U para identificar al proceso, verificar sus permisos y acceder a su tabla de descriptores de archivo.

3. **Interacción con memoria:**
   - El kernel consulta la tabla de regiones para encontrar la región de memoria donde se almacenará el contenido del archivo.
   - Realiza la operación de lectura y copia los datos en el buffer del proceso.

4. **Regreso a modo usuario:**
   - El kernel actualiza los registros del proceso y restaura su estado desde la pila del kernel.
   - El control vuelve al proceso en modo usuario.

#### **Resumen**
- **Pila del kernel:** Almacena el estado y las rutinas invocadas mientras el proceso está en modo kernel.
- **Área U:** Contiene información de control necesaria para la ejecución del proceso.
- **Tabla de regiones:** Gestiona las asignaciones de memoria del proceso, permitiendo al kernel controlar regiones de código, datos y pila.

Estas estructuras trabajan en conjunto para garantizar la correcta gestión y ejecución de los procesos en el sistema operativo.

# Hilos
En los sistemas operativos tradicionales, cada proceso tiene su propio **espacio de direcciones** y un solo **hilo de control**. Sin embargo, en algunos casos, resulta más conveniente tener varios hilos dentro del mismo proceso, lo que permite ejecutar tareas en paralelo dentro del mismo espacio de direcciones, lo que facilita el diseño de ciertas aplicaciones.

### Uso de hilos
**¿Por qué usar hilos dentro de un proceso?** La razón principal es que, al desarrollar aplicaciones, frecuentemente se tienen varias tareas que deben realizarse simultáneamente o en cuasi-paralelo, lo cual se puede hacer utilizando hilos. Aquí están las razones detalladas para usar hilos:

#### 1. **Simplicidad en el modelo de programación**
Cuando se descompone una aplicación en varios hilos, el modelo de programación se simplifica. En lugar de pensar en **interrupciones** y **conmutaciones de contexto**, que son típicos de los procesos, se puede pensar en **procesos paralelos**. Los hilos permiten que varias tareas se ejecuten en paralelo y, lo más importante, compartan el mismo **espacio de direcciones** (*es decir, accedan a los mismos datos*). Esto no sería posible si se usaran varios procesos, ya que cada proceso tiene su propio espacio de direcciones.

#### 2. **Eficiencia en la creación y destrucción**
Los hilos son **mucho más ligeros que los procesos**, lo que significa que son más rápidos de crear y destruir. En muchos sistemas, crear un hilo puede ser **de 10 a 100 veces más rápido que crear un proceso**. Esto es muy útil cuando el número de hilos que se necesitan cambia de forma dinámica y rápida.

#### 3. **Mejora en el rendimiento**
Aunque los hilos no aumentan el rendimiento cuando todos están limitados por la CPU, son muy útiles en situaciones donde hay operaciones de **entrada/salida (E/S)** o cálculos intensivos. Al tener hilos, las operaciones de E/S y los cálculos pueden realizarse en paralelo, **traslapándose**, lo que mejora el rendimiento de la aplicación.

#### 4. **Paralelismo real en sistemas multiprocesador**
Cuando se ejecutan aplicaciones en sistemas con varias CPUs (procesadores), los hilos permiten aprovechar el verdadero **paralelismo**, ya que cada hilo puede ejecutarse en diferentes CPUs al mismo tiempo, maximizando el uso de los recursos disponibles.

### Modelo clásico de hilo
El **modelo clásico de hilos** describe cómo los hilos permiten la ejecución concurrente dentro de un solo proceso. Un proceso es una entidad que agrupa recursos como espacio de direcciones, archivos abiertos, y otros recursos del sistema, mientras que un hilo se encarga de ejecutar el código. Aquí te explico los puntos clave:

1. **Separación de recursos y ejecución**: El modelo de procesos tradicional agrupa recursos (*como memoria y archivos*) y la ejecución (*hilos*). Los hilos permiten que varios flujos de ejecución se compartan en un mismo proceso, pero **cada uno de ellos tiene su propio contador de programa, pila y registros**, permitiendo que se realicen tareas concurrentemente dentro del mismo espacio de direcciones.

![[archivos/imagenes/Pasted image 20241117191436.png]]

![[archivos/imagenes/Pasted image 20241117191416.png]]

2. **Ejecución concurrente dentro de un proceso**: Cuando hay múltiples hilos dentro de un solo proceso, todos **comparten el mismo espacio de direcciones**, pero cada uno puede ejecutar una parte del código de manera independiente. Es como tener varios procesos, pero **sin tener que duplicar los recursos**.

![[archivos/imagenes/Pasted image 20241117191405.png]]

3. **Comparación con procesos**: A diferencia de los procesos, que están aislados en cuanto a sus espacios de direcciones y recursos, los hilos dentro de un proceso **comparten la memoria, archivos abiertos, señales, etc**. Esto significa que los hilos dentro de un mismo proceso pueden comunicarse fácilmente entre sí y colaborar en tareas sin necesidad de mecanismos complicados de comunicación entre procesos.

4. **Propiedades de los hilos**: Los hilos tienen un **contador de programa** (*que marca qué instrucción ejecutar a continuación*), **registros** (*para almacenar variables locales*), y una **pila** (*para gestionar el historial de llamadas de función*). Aunque varios hilos pueden compartir la misma memoria, cada hilo tiene su propia pila, para almacenar las variables locales de las funciones en ejecución.

5. **Conmutación de hilos**: Al igual que en la multiprogramación de procesos, la CPU **cambia rápidamente** entre los hilos para dar la impresión de que se están ejecutando en paralelo. Este tipo de ejecución es más eficiente que tener múltiples procesos completos corriendo, ya que los hilos comparten recursos, lo que reduce la sobrecarga de gestión de memoria.

6. **Estado de los hilos**: Los hilos pueden estar en distintos estados, como "en ejecución", "bloqueado" (*esperando un evento, como la lectura de un archivo*), o "listo" (*esperando su turno para ejecutarse*). Esto es similar al ciclo de vida de los procesos, pero con la particularidad de que todos los hilos comparten los mismos recursos.

7. **Interacción y sincronización entre hilos**: Como los hilos comparten recursos, es importante tener mecanismos para evitar problemas como el acceso concurrente a datos compartidos (por ejemplo, **un hilo cerrando un archivo mientras otro lo lee**). Las herramientas de sincronización, como los semáforos o los mutexes, son necesarias para evitar estos problemas.

8. **Desventajas y complicaciones**: Aunque los hilos son útiles, pueden generar complicaciones. Por ejemplo, si un proceso con varios hilos crea un nuevo proceso, ¿deben los hilos del padre ser duplicados en el hijo? También surgen problemas de sincronización, como si dos hilos intentan acceder a la misma memoria al mismo tiempo. Estos problemas deben resolverse mediante técnicas de programación cuidadosa.

> [!Importante]
> **VARIABLES:**
> - **Accesibles por todos los hilos**:
>    - Variables globales
>    - Archivos abiertos
>    - Memoria dinámica (heap)
>    - Otros recursos compartidos del sistema (como señales)
> - **Privadas a cada hilo**:
>    - Variables locales
>    - Pila del hilo (incluyendo el contexto de ejecución y las variables locales de las funciones)


Cuando se utiliza el modelo de **multihilamiento**, los procesos generalmente comienzan con **un solo hilo**. Este hilo inicial tiene la capacidad de crear nuevos hilos a través de una llamada a una función de la biblioteca, como **`thread_create`**. Esta función suele tomar un parámetro que especifica el **procedimiento** que debe ejecutar el nuevo hilo. Lo importante aquí es que **no es necesario (ni posible) especificar un espacio de direcciones nuevo para el hilo**, ya que el nuevo hilo se ejecutará dentro del **espacio de direcciones** del hilo que lo creó. Esto significa que todos los hilos comparten el mismo espacio de direcciones.

#### Jerarquía de hilos
Aunque en algunos casos los hilos pueden tener una **relación jerárquica** (padre e hijo), en muchos casos todos los hilos son **iguales** y no existe una estructura jerárquica. El hilo que crea a otros hilos (el hilo creador) generalmente recibe un **identificador único**, que es utilizado para identificar y referirse al nuevo hilo.

#### Terminación de hilos
Cuando un hilo termina su trabajo, puede **terminar su ejecución** mediante una llamada como **`thread_exit`**. Después de hacer esto, el hilo desaparece y ya no puede ser **replanificado** para ejecutarse nuevamente. En sistemas con hilos, también existe la posibilidad de que un hilo **espere** a que otro hilo termine antes de continuar. Esto se logra a través de la llamada **`thread_join`**, que bloquea el hilo que la invoca hasta que el hilo especificado haya finalizado su ejecución.

#### Ceder la CPU
Otra función importante es **`thread_yield`**, que permite a un hilo **ceder voluntariamente la CPU** para que otro hilo pueda ejecutarse. Dado que no hay un sistema de interrupciones de reloj como en la **multiprogramación de procesos**, es crucial que los hilos sean "amables" y cedan la CPU para que los demás hilos tengan la oportunidad de ejecutarse.

#### Problemas con el modelo de hilos
A pesar de los beneficios de los hilos, también surgen ciertos **problemas complejos** en su uso:

1. **Uso del sistema `fork` en UNIX**: Cuando un proceso con varios hilos invoca la llamada al sistema **`fork`**, puede ser necesario que el proceso hijo también herede todos los hilos del proceso padre. Esto podría generar **problemas** si un hilo del padre está bloqueado (por ejemplo, esperando una entrada desde el teclado) y el hijo también hereda ese bloqueo, lo que podría generar **inconsistencias** en el comportamiento del programa.

2. **Acceso a recursos compartidos**: Dado que los hilos comparten recursos como archivos abiertos, memoria dinámica, etc., pueden surgir **conflictos**. Por ejemplo:
   - Si un hilo cierra un archivo mientras otro hilo aún está leyendo de él, puede generarse un **error de acceso**.
   - Si un hilo detecta que hay poca memoria y empieza a asignar más memoria, y a la vez otro hilo también intenta hacer lo mismo, pueden producirse **duplicaciones** en la asignación de memoria.

Para resolver estos problemas, es necesario diseñar y gestionar cuidadosamente los **recursos compartidos** y las transiciones de los hilos para evitar **condiciones de carrera** y otros errores típicos en programas con múltiples hilos.

### Hilos en POSIX
Para crear programas con hilos portátiles, el **IEEE** definió el estándar **1003.1c**, conocido como **Pthreads**. Este paquete de hilos es soportado por la mayoría de los sistemas **UNIX** y define más de 60 funciones. A continuación, se describen algunas de las llamadas más importantes:

- **Propiedades de los hilos Pthreads**: Cada hilo tiene un **identificador**, un conjunto de **registros** (incluyendo el contador de programa) y un conjunto de **atributos**, que se almacenan en una estructura. Los atributos incluyen el tamaño de la pila, parámetros de planificación, entre otros elementos necesarios para la ejecución del hilo.

- **Creación de hilos**: Para crear un hilo, se utiliza la llamada a **`pthread_create`**. El identificador del hilo recién creado se devuelve como valor de la función.

- **Terminación de hilos**: Cuando un hilo termina su tarea, puede finalizar llamando a **`pthread_exit`**, lo que detiene su ejecución y libera su pila.

- **Esperar a otro hilo**: Un hilo puede esperar a que otro termine usando **`pthread_join`**, bloqueando al hilo que la llama hasta que el hilo especificado termine.

- **Ceder la CPU**: Un hilo puede ceder voluntariamente la CPU a otro hilo utilizando **`pthread_yield`**. Esto es útil cuando los hilos están trabajando en conjunto y un hilo desea darle oportunidad a otros para que se ejecuten.

- **Atributos del hilo**: Se pueden crear y modificar los atributos de un hilo con las funciones **`pthread_attr_init`** (inicializa la estructura de atributos) y **`pthread_attr_destroy`** (elimina la estructura de atributos).

#### Ejemplo de uso de Pthreads
En el siguiente ejemplo, el programa principal crea 10 hilos, cada uno de los cuales imprime un mensaje. El orden en que los mensajes se intercalan puede variar entre ejecuciones, ya que depende de la planificación de los hilos.

```c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#define NUMERO_DE_HILOS 10

void *imprimir_hola_mundo(void *tid) {
    printf("Hola mundo. Saludos del hilo %d\n", tid);
    pthread_exit(NULL);
}

int main(int argc, char *argv[]) {
    pthread_t hilos[NUMERO_DE_HILOS];
    int estado, i;
    for(i = 0; i < NUMERO_DE_HILOS; i++) {
        printf("Aqui main. Creando hilo %d\n", i);
        estado = pthread_create(&hilos[i], NULL, imprimir_hola_mundo, (void *)i);
        if (estado != 0) {
            printf("Ups. pthread_create devolvió el código de error %d\n", estado);
            exit(-1);
        }
    }
    exit(NULL);
}
```

#### Funciones clave de Pthreads

| **Llamada de hilo**          | **Descripción**                                   |
|------------------------------|---------------------------------------------------|
| `pthread_create`             | Crea un nuevo hilo                                |
| `pthread_exit`               | Termina el hilo llamador                          |
| `pthread_join`               | Espera a que un hilo específico termine          |
| `pthread_yield`              | Libera la CPU para dejar que otro hilo se ejecute|
| `pthread_attr_init`          | Crea e inicializa la estructura de atributos de un hilo |
| `pthread_attr_destroy`       | Elimina la estructura de atributos de un hilo    |

Aunque hay muchas más funciones disponibles en el paquete Pthreads, estas son algunas de las más comunes utilizadas para la gestión básica de hilos.

### Implementaciones
#### En el espacio de usuario
El núcleo no sabe nada sobre los hilos, que se implementan con una biblioteca. Esto permite hilos en sistemas operativos que no aceptan hilos, y requiere una tabla de hilos privada por cada proceso, administrada en tiempo de ejecución por ese proceso.
3
Cuando un hilo debe bloquearse, llama a un procedimiento en tiempo de ejecución, en el cual se comprueba si el hilo debe bloquearse. Si es necesario, se almacenan los registros del hilo en la tabla de hilos, se busca otro hilo listo para ejecutarse y se cargan los valores del nuevo hilo en los registros para comenzar su ejecución.

Como se puede ver, la conmutación entre hilos es más rápida que la conmutación entre procesos, ya que **no necesita un trap**, **vaciar la caché**, **cambiar el contexto**, etc. También permite el uso de algoritmos de planificación personalizados.

##### Problemas en este sistema:
- **Escalabilidad**: Una tabla de hilos central puede ser un problema cuando hay muchos hilos.
- **Llamadas al sistema con bloqueo**: Pueden parar todos los hilos en un proceso. Esto se soluciona comprobando antes si habrá bloqueo.
- **Fallos de página**: Si un hilo tiene un fallo de página, todos los hilos del proceso se bloquean.
- **Renuncia voluntaria a la CPU**: Los hilos deben renunciar a la CPU voluntariamente, ya que el SO no realiza la conmutación entre ellos.

![[archivos/imagenes/Pasted image 20241117210811.png]]

#### En el espacio de núcleo
En este caso, es el kernel quien administra los hilos, por lo que no hay tabla de hilos en cada proceso ni se necesita un sistema en tiempo de ejecución. El kernel almacena una tabla de hilos con el estado de cada uno.

Para gestionarlos, se requieren llamadas al sistema. Cuando un hilo se bloquea, el kernel decide si se ejecuta otro hilo del mismo proceso o un hilo de otro proceso.

##### Ventajas:
- **No se necesitan llamadas al sistema sin bloqueo**.
  
##### Desventajas:
- **Velocidad**: Las llamadas al sistema son más costosas, lo que hace que el sistema sea más lento.
- **Problemas adicionales**: El SO debe resolver problemas como cuántos hilos tiene un proceso creado con `fork` o qué hilo atiende las señales.

![[archivos/imagenes/Pasted image 20241117210852.png]]

#### Implementaciones híbridas
Es un intento de combinar las ventajas de los hilos en el espacio de usuario con las de los hilos en el espacio de núcleo. En este enfoque, se implementa un sistema de hilos en el kernel que se multiplexan en varios hilos de usuario. Es decisión del programador cuántos hilos de cada tipo utilizar.

Lógicamente, el SO solo será consciente de los hilos en el espacio de núcleo.

#### Hilos emergentes (Pop-up thread)
Los hilos emergentes se utilizan frecuentemente en sistemas distribuidos, que consisten en varias computadoras conectadas a través de una red, permitiendo que el usuario acceda a los recursos de todas las máquinas desde una sola.

El método tradicional es que un proceso esté bloqueado en una llamada al sistema `receive`, esperando un mensaje entrante. Cuando el mensaje llega, el proceso lo procesa y maneja. Sin embargo, es posible hacer que la llegada del mensaje cree un nuevo hilo para manejarlo. Este hilo que se crea se conoce como **hilo emergente**.

![[archivos/imagenes/Pasted image 20241117211021.png]]

#### Hilos en Linux
En Linux, los hilos se gestionan de manera especial debido a su capacidad para combinar la distinción tradicional entre procesos e hilos. Aunque Linux es un sistema operativo multitarea que permite ejecutar varios procesos de forma concurrente, también es un sistema multihilo que permite ejecutar múltiples hilos dentro de un mismo proceso. A través de la llamada al sistema `clone`, Linux permite crear hilos con un control fino sobre lo que se comparte entre hilos y lo que se mantiene privado.

##### La llamada al sistema `clone`
La función `clone` en Linux permite la creación de un nuevo hilo o proceso, dependiendo de los parámetros que se pasen. Esta llamada es muy flexible porque, a diferencia de otros sistemas operativos donde los hilos se gestionan de forma independiente de los procesos, `clone` permite a los programadores decidir qué recursos o características se comparten entre el hilo hijo y el hilo padre, y qué recursos se mantienen privados.

La función `clone` tiene la siguiente firma:

```c
int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...);
```

**Parámetros de la función:**

1. **`fn`**: Es un puntero a una función que será ejecutada por el hilo recién creado. Cuando el hilo hijo es creado, empieza su ejecución en esta función.
  
2. **`child_stack`**: Es un puntero a la pila que se utilizará para el nuevo hilo. Este parámetro es necesario para la creación de un hilo, ya que cada hilo necesita su propia pila de ejecución.

3. **`flags`**: Es un parámetro de control que define cómo se compartirán los recursos entre el hilo padre y el hilo hijo. Aquí se especifican las propiedades que deben ser compartidas o privadas. Por ejemplo:
   - `CLONE_VM`: Comparte el espacio de direcciones de memoria (por lo tanto, los hilos comparten memoria).
   - `CLONE_FS`: Comparte la información del sistema de archivos.
   - `CLONE_FILES`: Comparte los descriptores de archivos.
   - `CLONE_SIGHAND`: Comparte las señales.
   - Y otros flags que permiten un control detallado sobre la creación del hilo.

4. **`arg`**: Es un argumento que se pasa a la función `fn` cuando se ejecuta. Puede ser cualquier dato que se desee pasar al hilo hijo al momento de su creación.

5. **Otros parámetros**: Dependiendo de la versión y de la plataforma, puede haber otros parámetros opcionales.

###### Características clave de `clone`:
- **Creación de hilos ligeros**: Con `clone`, se puede crear un hilo ligero (hilo en el sentido de ejecución concurrente) dentro de un proceso o crear un nuevo proceso. Si se desea que el hilo compartido utilice un espacio de memoria común, se puede usar la bandera `CLONE_VM`.
  
- **Flexibilidad en el manejo de recursos**: La principal ventaja de `clone` es que se pueden compartir o no ciertos recursos como memoria, descriptores de archivo, y más. Esto otorga gran flexibilidad para el manejo de la concurrencia en Linux.
  
- **Control sobre la pila**: Cada hilo tiene su propia pila de ejecución, lo que asegura que no haya colisiones entre los hilos en cuanto a los datos locales de cada uno. El parámetro `child_stack` especifica dónde debe estar ubicada la pila del hilo hijo.

###### Ejemplo básico de uso de `clone`:

```c
#include <stdio.h>
#include <sched.h>
#include <unistd.h>

int child_func(void *arg) {
    printf("Soy el hilo hijo con argumento: %d\n", *(int*)arg);
    return 0;
}

int main() {
    int arg = 42;
    char stack[1024 * 1024]; // Reserva espacio para la pila del hilo hijo
    pid_t pid = clone(child_func, stack + sizeof(stack), CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND, &arg);

    if (pid == -1) {
        perror("clone");
        return 1;
    }

    printf("Soy el hilo padre\n");

    return 0;
}
```

En este ejemplo:
- Se crea un hilo hijo que ejecuta la función `child_func`.
- El hilo hijo recibe el argumento `arg`, que se pasa desde el hilo padre.
- Se utiliza `clone` con los flags `CLONE_VM`, `CLONE_FS`, `CLONE_FILES`, y `CLONE_SIGHAND`, lo que indica que el hilo hijo comparte recursos importantes con el hilo padre.

##### Conclusión
La llamada al sistema `clone` en Linux proporciona una forma muy poderosa y flexible para crear hilos y procesos, permitiendo decidir qué recursos se comparten y cuáles se mantienen separados. Esto permite a los programadores tener un control detallado sobre la creación de hilos, la gestión de memoria y otros recursos, y es la base de la implementación de hilos en Linux.

# Planificación
La **planificación** es una función esencial en sistemas operativos multiprogramados, donde múltiples procesos o hilos compiten por la CPU. El **planificador** decide qué proceso se ejecutará según un **algoritmo de planificación**, optimizando el uso del procesador y garantizando un rendimiento eficiente.

#### Conceptos clave:
1. **Competencia por la CPU**:
   - Los procesos en **estado listo** compiten por la CPU.
   - En PCs simples con pocos procesos, la planificación tiene menor impacto.
   - En servidores, la planificación es crítica para aprovechar al máximo la CPU y minimizar el desperdicio de recursos.

2. **Clasificación de procesos**:
   - **Limitados por CPU (cálculos)**:
     - Ráfagas largas de cálculo con pocas esperas por E/S.
   - **Limitados por E/S**:
     - Ráfagas cortas de CPU con frecuentes operaciones de E/S.
   - El **factor clave** es la duración de la ráfaga de CPU. Procesos limitados por E/S necesitan obtener rápidamente la CPU para mantener otros dispositivos ocupados.

![[archivos/imagenes/Pasted image 20241117222127.png]]

3. **Ejemplo de uso eficiente de la CPU**:
   - Si el 80% del tiempo de un proceso se dedica a E/S, debe haber **10 procesos en memoria** para mantener el desperdicio de la CPU por debajo del 10%.
   - Si solo el 20% del tiempo es de E/S, bastan **2 procesos en memoria** para mantener un uso eficiente de la CPU.

4. **Preguntas esenciales para la planificación**:
   - ¿Debe ejecutarse el proceso **padre** o el **hijo** al crear un nuevo proceso?
   - ¿Qué proceso listo debe ejecutarse tras la **terminación** de otro?
     - Si no hay procesos listos, se ejecuta un proceso **inactivo**.
   - Si un proceso se **bloquea**, ¿se debe considerar la razón del bloqueo?
   - Ante una **interrupción de E/S**, ¿debe ejecutarse el proceso que estaba esperando?
   - Si ocurre una **interrupción de reloj**, ¿se cambia de proceso?

5. **Categorías de algoritmos de planificación**:
   - **No apropiativo**:
     - Selecciona un proceso y lo ejecuta hasta que este se bloquee o termine.
   - **Apropiativo**:
     - Selecciona un proceso y lo ejecuta durante un **tiempo máximo fijo**. Si el proceso excede este tiempo, se suspende y se da paso a otro.

6. **Eficiencia del planificador**:
   - Minimizar cambios de contexto para reducir el costo de las conmutaciones.
   - Responder rápidamente a los eventos (bloqueos, interrupciones) para garantizar un flujo de ejecución continuo.

En conclusión, la planificación permite equilibrar la ejecución de procesos o hilos, asegurando que la CPU se utilice de manera eficiente, tanto en sistemas simples como en entornos complejos como servidores.

### Categorías y Metas de Algoritmos de Planificación
#### Categorías de Algoritmos de Planificación
1. **Procesamiento por lotes**:  
   - Uso común en tareas empresariales periódicas (nóminas, inventarios, etc.).
   - Se priorizan algoritmos **no apropiativos** o con largos períodos de ejecución para reducir conmutaciones y maximizar el rendimiento.

2. **Sistemas interactivos**:  
   - Diseñados para múltiples usuarios con demandas simultáneas.
   - Requieren algoritmos **apropiativos** para garantizar respuesta rápida y evitar procesos que monopolizan la CPU.

3. **Sistemas de tiempo real**:  
   - Prioridad en cumplir con plazos estrictos (e.g., recolección de datos de sensores).
   - Pueden prescindir de apropiación si los procesos cooperan y se bloquean rápidamente.

---

#### Metas de los Algoritmos de Planificación 
1. **Metas generales** (aplicables a todos los sistemas):  
   - **Equidad**: Todos los procesos deben recibir un trato justo.  
   - **Aplicación de políticas**: Garantizar que se cumplan las prioridades establecidas.  
   - **Balance**: Mantener ocupadas la CPU y los dispositivos de E/S para maximizar el uso del sistema.

2. **Metas específicas por entorno**:  
   - **Procesamiento por lotes**:  
     - **Rendimiento**: Maximizar trabajos completados por hora.  
     - **Tiempo de retorno**: Minimizar el tiempo entre envío y finalización.  
     - **Utilización de la CPU**: Mantenerla siempre ocupada.

   - **Sistemas interactivos**:  
     - **Tiempo de respuesta**: Responder rápidamente a las solicitudes.  
     - **Proporcionalidad**: Cumplir con las expectativas del usuario respecto a tiempos.

   - **Sistemas de tiempo real**:  
     - **Cumplir con plazos**: Garantizar ejecución dentro de límites temporales.  
     - **Predictibilidad**: Evitar degradaciones en tareas multimedia (e.g., calidad de audio/video).  

En conclusión, la planificación se adapta a las características y metas específicas de cada entorno, garantizando eficiencia, equidad y cumplimiento de prioridades.

### Planificación en Sistemas de Procesamiento por Lotes
Los sistemas de procesamiento por lotes suelen emplear algoritmos diseñados para maximizar la eficiencia del sistema, minimizando el tiempo de retorno y optimizando el uso de recursos. Aquí se analizan tres algoritmos populares:

#### **1. Primero en Entrar, Primero en Ser Atendido (FCFS, First-Come, First-Served)**
- **Descripción**:  
  - Es el algoritmo más simple y no apropiativo.  
  - Los procesos se ejecutan en el orden en que llegan, permaneciendo en ejecución hasta completarse.  
  - Usa una cola única donde los procesos listos se ordenan por llegada.  

- **Ventajas**:  
  - Fácil de entender e implementar.  
  - Justo en el sentido de atender los procesos según su orden de llegada.  

- **Desventajas**:  
  - **Efecto convoy**: Procesos cortos pueden esperar mucho si están detrás de un proceso largo.  
  - Ineficiente para sistemas con una mezcla de procesos intensivos en CPU y en E/S, pues los procesos de E/S pueden quedar inactivos demasiado tiempo.  

---

#### **2. El Trabajo Más Corto Primero (SJF, Shortest Job First)**  
- **Descripción**:  
  - No apropiativo.  
  - Selecciona el trabajo con el tiempo de ejecución más corto entre los procesos listos para ejecutarse.  
  - Requiere que se conozca de antemano la duración de los procesos.  

- **Ventajas**:  
  - Minimiza el tiempo promedio de respuesta cuando todos los trabajos están disponibles al mismo tiempo.  
  - Favorece a procesos cortos, reduciendo tiempos de espera.  

- **Desventajas**:  
  - No es práctico si no se pueden predecir los tiempos de ejecución.  
  - Puede causar **injusticia** hacia procesos largos que esperan indefinidamente.  

- **Ejemplo**:  
  Consideremos 4 procesos con tiempos de ejecución de 8, 4, 4 y 4 minutos.  
  - En orden de llegada (A, B, C, D), los tiempos de respuesta promedio serían:  
    - A: 8, B: 12, C: 16, D: 20 → **Promedio: 14 minutos**.  
  - Con SJF (B, C, D, A):  
    - B: 4, C: 8, D: 12, A: 20 → **Promedio: 11 minutos**.  

![[archivos/imagenes/Pasted image 20241117222921.png]]

#### **3. Menor Tiempo Restante a Continuación (SRTN, Shortest Remaining Time Next)**
- **Descripción**:  
  - Versión apropiativa del SJF.  
  - Siempre selecciona el proceso con el menor tiempo de ejecución restante.  
  - Si llega un nuevo proceso con un tiempo de ejecución menor al restante del proceso actual, se realiza un cambio de contexto para atender el nuevo proceso.  

- **Ventajas**:  
  - Beneficia a trabajos cortos que llegan mientras otro proceso está en ejecución.  
  - Ofrece una buena respuesta promedio para sistemas dinámicos.  

- **Desventajas**:  
  - Requiere conocer los tiempos de ejecución de antemano.  
  - Mayor sobrecarga por los frecuentes cambios de contexto.  
#### Conclusión  
- **FCFS** es simple y justo, pero puede ser ineficiente con cargas mixtas.  
- **SJF** y **SRTN** ofrecen menores tiempos de respuesta promedio, pero requieren estimaciones precisas de los tiempos de ejecución.  
- La elección del algoritmo depende del entorno específico y de los objetivos del sistema.

### Planificación en Sistemas Interactivos
Los sistemas interactivos requieren algoritmos que ofrezcan una buena experiencia de usuario, garantizando tiempos de respuesta cortos y priorizando tareas dependiendo de su urgencia. Aquí se analizan dos métodos clave: **Planificación por Turno Circular** y **Planificación por Prioridad**.
#### **1. Planificación por Turno Circular (Round-Robin)**
- **Descripción**:  
  - Cada proceso recibe un tiempo fijo (llamado *quántum*) para ejecutarse.  
  - Si el proceso no termina dentro del quántum, se pausa y pasa al final de la cola, permitiendo que otro proceso se ejecute.  
  - Es apropiativo y se implementa fácilmente con una lista circular de procesos.  

- **Ventajas**:  
  - Es simple, equitativo y asegura que ningún proceso quede excluido.  
  - Adecuado para sistemas interactivos, donde las tareas suelen ser cortas.  

- **Desventajas**:  
  - La eficiencia depende del tamaño del quántum:  
    - **Quántum corto**: Demasiados cambios de contexto, desperdiciando tiempo.  
    - **Quántum largo**: Tiempos de respuesta elevados para tareas interactivas cortas.  
  - No considera la prioridad o la naturaleza del proceso.  

- **Punto crítico**:  
  - Un valor comúnmente aceptado para el quántum oscila entre **20 y 50 ms**, equilibrando eficiencia y tiempos de respuesta.  

![[archivos/imagenes/Pasted image 20241117224134.png]]

#### **2. Planificación por Prioridad**
- **Descripción**:  
  - Cada proceso recibe una prioridad, y el sistema siempre ejecuta el proceso con la prioridad más alta.  
  - Puede ser **estática** (determinada al inicio) o **dinámica** (cambia durante la ejecución).  

- **Ventajas**:  
  - Procesos críticos o urgentes obtienen la CPU más rápidamente.  
  - Puede adaptarse a diferentes necesidades de un sistema multiusuario o multitarea.  

- **Desventajas**:  
  - **Inanición (Starvation)**: Procesos de baja prioridad pueden quedar sin ejecutarse si siempre hay procesos de alta prioridad.  
  - La implementación de prioridades dinámicas puede ser compleja.  

- **Ejemplo práctico**:  
  - En sistemas UNIX, el comando `nice` permite a los usuarios reducir voluntariamente la prioridad de sus procesos, aunque rara vez se utiliza.  
  - Las prioridades dinámicas pueden ajustarse en función del comportamiento del proceso:  
    - Por ejemplo, un proceso que usa solo una fracción de su quántum recibe una prioridad mayor, lo que favorece procesos limitados por E/S.  

---

#### **Combinación de Métodos**
A menudo se utilizan combinaciones de estos métodos para equilibrar equidad y eficiencia:  
- **Clases de Prioridad**:  
  - Los procesos se agrupan en clases de prioridad.  
  - Dentro de cada clase, los procesos se manejan por turno circular.  
  - Ejemplo:  
    1. Ejecutar procesos de prioridad 4 (máxima) en turno circular.  
    2. Si no hay procesos en prioridad 4, pasar a la prioridad 3, y así sucesivamente.  

#### Otros métodos de planificación
1. **Proceso más corto a continuación**:
   - Ejecuta el proceso con el tiempo estimado de ejecución más corto basado en estimaciones de ejecuciones previas (usando promedios ponderados). Esto minimiza el tiempo de respuesta total.

2. **Planificación garantizada**:
   - Cada usuario o proceso recibe una porción justa de la CPU proporcional al número de usuarios o procesos activos, asegurando equidad en la distribución de recursos.

3. **Planificación por sorteo**:
   - Los procesos reciben boletos de lotería; se elige al azar un ganador para asignar recursos como tiempo de CPU. Más boletos implican mayor probabilidad de selección, permitiendo flexibilidad y equidad probabilística.

4. **Planificación por partes equitativas**:
   - Divide el tiempo de CPU entre usuarios en lugar de procesos. Esto garantiza que todos los usuarios obtengan su parte justa, independientemente del número de procesos que ejecuten.

Cada técnica se adapta a necesidades específicas, como minimizar tiempos de respuesta, garantizar equidad o simplificar la implementación.

#### Conclusión  
- **Turno Circular** es ideal para sistemas donde la equidad es clave y las tareas tienen tiempos similares.  
- **Prioridades** son esenciales cuando algunos procesos requieren atención urgente o son más importantes.  
- Una combinación de ambos puede ofrecer un buen equilibrio para sistemas interactivos complejos.


### Planificación de Hilos
Cuando los sistemas manejan procesos con múltiples hilos, hay dos enfoques principales para su planificación, dependiendo de si los hilos están gestionados a nivel **usuario** o a nivel **kernel**. Cada método tiene sus características y limitaciones.

#### **1. Hilos a nivel usuario**
- **Características:**
  - Los hilos son gestionados completamente por el proceso, sin intervención del kernel.
  - El kernel selecciona un **proceso completo** para ejecutarlo y no sabe qué hilos existen dentro del proceso.
  - El planificador de hilos del proceso decide cuál hilo ejecutará dentro del tiempo asignado al proceso.

- **Ventajas:**
  - **Rendimiento alto:** Cambiar de un hilo a otro dentro del proceso es rápido, ya que no implica un cambio de contexto a nivel del kernel.
  - **Flexibilidad:** El proceso puede implementar su propio algoritmo de planificación, adaptado a sus necesidades.

- **Limitaciones:**
  - **Sin interrupciones por reloj:** Los hilos no pueden ser interrumpidos automáticamente, lo que puede llevar a que un hilo monopolice el tiempo del proceso.
  - **Bloqueo de E/S:** Si un hilo se bloquea esperando una operación de E/S, **todo el proceso se suspende**, ya que el kernel no distingue entre hilos.

#### **2. Hilos a nivel kernel**
- **Características:**
  - El kernel gestiona cada hilo individualmente, tratándolos como unidades independientes.
  - El kernel asigna un quántum de tiempo a cada hilo, sin importar a qué proceso pertenezca.
  - Si un hilo se bloquea, otros hilos (incluso del mismo proceso) pueden seguir ejecutándose.

- **Ventajas:**
  - **Mejor manejo de bloqueos:** Si un hilo espera E/S, otros hilos del mismo proceso pueden continuar ejecutándose.
  - **Planificación más general:** El kernel puede gestionar los hilos de todos los procesos con un enfoque unificado.

- **Limitaciones:**
  - **Rendimiento más bajo:** Cambiar entre hilos a nivel kernel requiere una conmutación de contexto completa, lo que es más lento debido a cambios en el mapa de memoria y la invalidación de la caché.
  - **Menos optimización específica:** El kernel no tiene conocimiento detallado de la lógica de la aplicación, lo que limita la capacidad de optimizar la planificación de hilos según las necesidades específicas de un programa.

![[archivos/imagenes/Pasted image 20241117224945.png]]

#### **Decisiones de planificación**
- En sistemas con hilos a nivel usuario:
  - El planificador del proceso puede decidir qué hilo ejecutar basándose en la lógica de la aplicación (por ejemplo, priorizar un hilo despachador en un servidor web).

- En sistemas con hilos a nivel kernel:
  - El kernel puede optimizar el uso de recursos al considerar costos como el cambio de contexto. Por ejemplo:
    - Dar preferencia a un hilo dentro del mismo proceso que otro hilo recientemente bloqueado para minimizar cambios de mapa de memoria.

#### **Conclusión**
La elección entre hilos a nivel usuario y hilos a nivel kernel depende de las prioridades del sistema:
- **Nivel usuario:** Mayor control y rendimiento para aplicaciones específicas.
- **Nivel kernel:** Mayor flexibilidad y capacidad para manejar bloqueos, aunque a costa de un rendimiento menor.