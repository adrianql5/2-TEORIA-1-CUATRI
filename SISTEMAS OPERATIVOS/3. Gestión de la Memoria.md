[[archivos/bibliografia/2-Sistemas operativos moderno 3ed Tanenbaum.pdf|2-Sistemas operativos moderno 3ed Tanenbaum]]

Escrito por **Adrián Quiroga Linares**.

La **memoria principal** (*RAM*) es un recurso esencial que debe ser administrado cuidadosamente. Aunque las computadoras actuales tienen mucha más memoria que las de hace décadas, los programas crecen a un ritmo más rápido que la capacidad de las memorias. Se dice que **"los programas se expanden para llenar la memoria disponible"**, lo que presenta desafíos para su gestión. A pesar de que los programadores desearían tener una memoria infinita, rápida, no volátil y económica, la tecnología actual no ofrece tales soluciones.

En lugar de esto, las computadoras tienen una **jerarquía de memoria**, que incluye **memoria caché** (*rápida, costosa y volátil*), **memoria principal** (*de velocidad media y precio moderado*), almacenamiento en **disco** (*lento, económico y no volátil*), y **almacenamiento removible**. El sistema operativo debe gestionar esta jerarquía de manera eficiente. Esto se logra a **través del administrador de memoria, encargado de asignar y liberar memoria según sea necesario**.

![[archivos/imagenes/Pasted image 20241121183519.png]]

# Sin abstracción de la memoria: 
En los sistemas más antiguos y simples, como las primeras computadoras mainframes (antes de 1960), minicomputadoras (antes de 1970) y computadoras personales (antes de 1980), no existía **abstracción de memoria**. Los programas interactuaban directamente con la **memoria física**, lo que significa que cada programa veía un conjunto lineal de direcciones físicas desde 0 hasta un valor máximo, donde cada dirección correspondía a una celda con un tamaño fijo de bits (típicamente 8 bits, un byte). Esto significaba:

1. **Acceso directo**: Si un programa ejecutaba una instrucción como `MOV REGISTRO1, 1000`, simplemente accedía a la ubicación física 1000 en la memoria.
2. **No había separación entre programas**: Si dos programas intentaban usar la misma dirección de memoria, podían sobrescribir los datos del otro, lo que resultaba en fallos inmediatos.

#### Organización de la memoria en sistemas sin abstracción

A pesar de no tener abstracción de memoria, existían diversas formas de organizar la memoria en estos sistemas, como se muestra en la figura 3-1:

1. **Sistema operativo en RAM (Figura 3-1a)**:
    
    - El sistema operativo residía en la parte inferior de la memoria RAM.
    - Este modelo era común en las primeras mainframes y minicomputadoras.
    - **Problema**: Un error en el programa del usuario podía sobrescribir el sistema operativo.
2. **Sistema operativo en ROM (Figura 3-1b)**:
    
    - El sistema operativo se colocaba en la parte superior de la memoria, en una ROM.
    - Común en sistemas integrados y computadoras de bolsillo.
    - La ROM protegía el sistema operativo de ser modificado, ya que era de solo lectura.
3. **Sistema operativo dividido entre RAM y ROM (Figura 3-1c)**:
    
    - El sistema operativo principal estaba en RAM, y los controladores de dispositivos en ROM.
    - Ejemplo: Las primeras computadoras personales, como las que usaban MS-DOS.
    - **BIOS (Basic Input Output System)**: Parte del sistema operativo en ROM.

![[archivos/imagenes/Pasted image 20241121183354.png]]

#### Limitaciones de los sistemas sin abstracción

1. **Ejecutar un solo programa a la vez**:
    
    - Un programa se cargaba en memoria y se ejecutaba.
    - Al terminar, se cargaba otro programa, sobrescribiendo al primero.
    - El sistema era secuencial: el usuario introducía comandos y esperaba a que cada programa finalizara.
2. **Conflictos entre programas**:
    
    - Sin abstracción, un programa podía borrar o corromper datos del sistema operativo o de otro programa.
3. **Falta de paralelismo**:
    
    - La única forma de lograr cierto paralelismo era usando múltiples **hilos** dentro del mismo programa. Sin embargo, este enfoque tenía limitaciones, ya que los hilos comparten la misma memoria y no permiten ejecutar programas independientes simultáneamente.

#### Soluciones para múltiples programas sin abstracción

Aunque no había abstracción de memoria, se desarrollaron estrategias para permitir la ejecución de varios programas:

1. **Intercambio de programas**:
    
    - Cuando un programa terminaba o se pausaba, su contenido en memoria se guardaba en disco.
    - Otro programa se cargaba desde el disco a la memoria, eliminando conflictos.
2. **Protección de memoria con hardware**:
    
    - En la IBM 360, la memoria se dividía en bloques de 2 KB con **llaves de protección** de 4 bits.
    - Cada proceso tenía una llave que debía coincidir con la del bloque de memoria que intentaba acceder. Si no coincidían, el hardware generaba una interrupción (trap).
    - Este enfoque evitaba que los procesos interfieran entre sí o con el sistema operativo.

#### Problema de reubicación

Un gran desafío era la **reubicación de direcciones** cuando varios programas compartían la memoria:

1. **Referencias absolutas**:
    
    - Cada programa usaba direcciones absolutas. Si un programa esperaba encontrar una instrucción en la dirección 24, no podía funcionar correctamente si se cargaba en otra ubicación.
2. **Reubicación estática**:
    
    - Para resolver esto, durante la carga del programa se sumaba un valor constante a todas las direcciones del programa, ajustándolas según la posición en memoria.
    - Problemas:
        - Era lento, ya que cada dirección debía ajustarse manualmente.
        - Requería información adicional en los programas para distinguir entre direcciones (que necesitaban ajuste) y constantes (que no debían modificarse).

![[archivos/imagenes/Pasted image 20241121183424.png]]

#### Aplicaciones modernas de sistemas sin abstracción

Aunque en los sistemas modernos (como mainframes y PCs) la memoria física ya no se utiliza directamente, en sistemas simples como dispositivos integrados (p. ej., radios, lavadoras, microondas), el software sigue utilizando **memoria física absoluta**. Razones:

- Los programas se conocen de antemano y son específicos para el dispositivo.
- Los usuarios no pueden cargar su propio software.

En algunos casos, un sistema operativo mínimo actúa como una biblioteca para facilitar tareas comunes, como el sistema operativo **e-cos**.


# Espacios de Direcciones
Exponer directamente la memoria física a los procesos tiene desventajas, principalmente:

1. **Problemas de protección:** Los procesos pueden alterar accidental o intencionalmente el sistema operativo.
2. **Dificultad para ejecutar múltiples programas:** Es complejo mantener varios programas en memoria al mismo tiempo sin interferencias.

Para resolver esto, se introduce una **abstracción de memoria** conocida como el **espacio de direcciones**.

---
### **La noción de un espacio de direcciones**
Un **espacio de direcciones** es el conjunto de direcciones que un proceso puede utilizar para acceder a la memoria. Cada proceso tiene su propio espacio, independiente de los demás, salvo casos específicos de compartición.

Este concepto se aplica a múltiples contextos, como:

- **Direcciones IP:** Espacio de 32 bits (IPv4: 0 a 232−12^{32}-1).
- **Puertos de E/S:** Espacio de direcciones de 0 a 16,383 en el Pentium.
- **Dominios de Internet:** Cadenas alfanuméricas, como en ".com".

El desafío consiste en asignar a cada programa un espacio de direcciones que corresponda a distintas ubicaciones físicas sin interferencias.

---
#### **Registros base y límite**
Una solución clásica utiliza **reubicación dinámica** con registros especiales en el hardware de la CPU:

1. **Registro base:** Contiene la dirección física donde empieza el programa.
2. **Registro límite:** Define la longitud del programa.

Cuando un proceso accede a la memoria:
- El hardware suma automáticamente el valor del registro base a la dirección generada por el programa.
- Verifica si la dirección resultante excede el rango permitido (base + límite). Si es así, genera un error y aborta el acceso.

##### **Ventajas:**
- Permite a cada proceso un espacio de direcciones privado.
- Protege los registros base y límite, para que solo el sistema operativo pueda modificarlos.

##### **Desventajas:**
- Se requiere realizar una suma y una comparación en cada acceso a memoria, lo que puede ser lento sin circuitos especiales.

![[archivos/imagenes/Pasted image 20241121185123.png]]
---

#### **Conclusión**
Los espacios de direcciones permiten a los sistemas operativos abstraer y gestionar la memoria de forma segura y eficiente, haciendo posible la ejecución de múltiples programas simultáneamente sin interferencias. Este concepto evolucionó con el tiempo hacia métodos más avanzados en los procesadores modernos.

### Itercambio
El intercambio (_swapping_) es una estrategia de administración de memoria que permite a los sistemas operativos ejecutar procesos incluso cuando no hay suficiente memoria física disponible para alojarlos todos al mismo tiempo. Se basa en mover procesos completos entre la memoria principal (RAM) y el almacenamiento secundario (disco) según sea necesario.

#### **Motivación para el Intercambio**
1. **Cantidad limitada de RAM:** En sistemas modernos, la cantidad total de RAM necesaria para ejecutar todos los procesos concurrentemente puede exceder la memoria disponible.
    
    - Por ejemplo, aplicaciones de usuario como navegadores o editores de video pueden consumir entre 50 y 200 MB cada una.
    - Además, procesos en segundo plano como actualizaciones de software o comprobaciones de correo ocupan memoria, incluso antes de que el usuario ejecute sus propias aplicaciones.
2. **Requerimientos dinámicos:** Los procesos pueden necesitar más memoria durante su ejecución debido a estructuras dinámicas como el _heap_ o la pila.



#### **Cómo Funciona el Intercambio**
1. **Movimiento de procesos:**
    - Cuando un proceso está listo para ejecutarse, se carga completamente en la memoria.
    - Una vez que ha usado su tiempo de CPU asignado o se encuentra inactivo, puede ser devuelto al disco, liberando espacio en la memoria.

2. **Ejemplo de Operación:**
    - Inicialmente, sólo el proceso A está en la memoria.
    - Luego, se cargan los procesos B y C, desplazando a A si es necesario.
    - Más tarde, A regresa a memoria en una ubicación diferente, requiriendo que sus direcciones se ajusten.

3. **Reubicación de direcciones:**
    - Cuando un proceso se mueve, las direcciones internas que usa deben ser actualizadas.
    - Esto puede realizarse mediante:
        - **Software:** Reescribiendo las direcciones durante el intercambio.
        - **Hardware:** Usando registros base y límite para traducir las direcciones durante la ejecución.


![[archivos/imagenes/Pasted image 20241121185439.png]]

#### **Desafíos del Intercambio**
1. **Fragmentación de la memoria:**
    - Con el tiempo, el intercambio puede generar espacios vacíos dispersos (fragmentación externa).
        
    - Una técnica para solucionar esto es la **compactación de memoria**, donde los procesos se agrupan desplazándolos hacia abajo en la memoria.
        
    - Sin embargo, este proceso es costoso en términos de tiempo de CPU.
        
    
    > Por ejemplo, en un sistema con 1 GB de RAM y un ancho de banda de 4 bytes cada 20 nanosegundos, la compactación completa tomaría cerca de 5 segundos.
    
2. **Asignación de memoria adicional:**
    - Cuando se espera que los procesos crezcan (por ejemplo, usando _heap_), se puede asignar espacio extra para evitar moverlos con frecuencia.
    - Si un proceso crece más allá del espacio asignado, puede:
        - Usar un hueco adyacente.
        - Moverse a otro lugar en la memoria.
        - Intercambiarse al disco hasta que haya espacio disponible.
3. **Eficiencia:**
    - Sólo debe intercambiarse al disco la memoria realmente en uso por el proceso, ya que intercambiar espacio extra es ineficiente.

#### **Esquemas para Crecimiento de Procesos**
1. **Crecimiento de segmentos únicos:**
    - Si sólo se espera que crezca un segmento (por ejemplo, datos), se reserva espacio adicional entre procesos.

2. **Crecimiento de múltiples segmentos:**
    - En lenguajes de programación que usan tanto _heap_ como pila, ambos pueden crecer simultáneamente en direcciones opuestas:
        - _Heap_ crece hacia arriba.
            
        - _Pila_ crece hacia abajo.
            
    
    > Si el espacio reservado entre ambos se agota, el proceso deberá moverse o intercambiarse.

![[archivos/imagenes/Pasted image 20241121185557.png]]

#### **Conclusión**
El intercambio es una solución efectiva cuando la memoria física no es suficiente para contener todos los procesos. Sin embargo, tiene limitaciones debido al tiempo de procesamiento requerido para mover procesos, gestionar fragmentación y realizar compactación. Esta estrategia sentó las bases para la **memoria virtual**, que ofrece mayor flexibilidad al permitir que procesos se ejecuten parcialmente en memoria, tema que se aborda en la sección siguiente.

### Administración de la memoria libre
Cuando la memoria se asigna dinámicamente, el sistema operativo administra el espacio disponible mediante dos métodos principales: **mapas de bits** y **listas libres**. Cada método tiene ventajas y desventajas según el diseño y las necesidades del sistema.

#### **1. Administración de Memoria con Mapas de Bits**
En este método:
- **División de la memoria**: La memoria se divide en unidades de asignación. Estas unidades pueden ser pequeñas (unas palabras) o grandes (varios kilobytes).
- **Mapa de bits**: Por cada unidad de memoria, existe un bit en el mapa:
    - **0**: Indica que la unidad está libre.
    - **1**: Indica que la unidad está ocupada.

**Ejemplo**: La memoria de la figura 3-6(a) tiene procesos y huecos representados como bloques. Su mapa de bits correspondiente es una secuencia de bits, donde las áreas sombreadas indican unidades libres.

**Diseño del mapa**:
- El tamaño del mapa depende del tamaño de la memoria y de la unidad de asignación.
- Ejemplo: Con unidades de 4 bytes, un mapa de bits requiere 1 bit por cada 4 bytes de memoria. Esto equivale a que el mapa ocupa 1/33 del tamaño total de la memoria.

**Desafíos**:
1. **Búsqueda lenta**: Encontrar un bloque libre de tamaño kk implica buscar en el mapa hasta hallar kk bits consecutivos en 0. Esto puede cruzar límites entre palabras del mapa, volviendo el proceso más lento.
2. **Fragmentación interna**: Si el proceso no ocupa un múltiplo exacto de la unidad de asignación, el espacio sobrante se desperdicia.

![[archivos/imagenes/Pasted image 20241121190131.png]]

#### **2. Administración de Memoria con Listas Ligadas**
En este método:
- **Representación**: La memoria se gestiona como una lista ligada de segmentos. Cada segmento puede ser un proceso o un hueco vacío.
    - **Nodo de la lista**: Contiene información sobre el tipo (proceso o hueco), la dirección inicial, la longitud y un puntero al siguiente segmento.

**Ventajas**:
- La lista se mantiene ordenada por direcciones, lo que facilita actualizaciones cuando un proceso termina o es intercambiado.

**Gestión de huecos**: Cuando un proceso finaliza, su espacio se convierte en un hueco. Esto implica cuatro posibles combinaciones según los vecinos (ver figura 3-7):
1. Un único proceso vecino se reemplaza por un hueco.
2. Dos entradas (un hueco y un proceso) se fusionan.
3. Se eliminan dos entradas y se crea un hueco mayor.
4. Tres entradas (dos huecos y un proceso) se combinan en una sola.

![[archivos/imagenes/Pasted image 20241121190250.png]]

#### **3. Algoritmos de Asignación**
Los algoritmos más comunes son:

1. **Primer Ajuste**:
    - Recorre la lista hasta encontrar el primer hueco suficientemente grande.
    - Es rápido porque no busca más allá del primer hueco adecuado.
2. **Siguiente Ajuste**:    
    - Similar al primer ajuste, pero empieza a buscar desde donde quedó la última búsqueda, no desde el principio.
    - Es ligeramente menos eficiente que el primer ajuste.
3. **Mejor Ajuste**:
    - Busca en toda la lista el hueco más pequeño que sea suficiente.
    - Ventaja: Reduce el desperdicio al dividir grandes huecos innecesariamente.
    - Desventaja: Es más lento y puede fragmentar la memoria con huecos pequeños e inutilizables.
4. **Peor Ajuste**:
    - Selecciona siempre el hueco más grande disponible.
    - Es poco eficiente en la práctica, ya que crea fragmentación rápidamente.
5. **Ajuste Rápido**:
    - Mantiene listas separadas para tamaños comunes (e.g., 4 KB, 8 KB, etc.).
    - Es muy rápido, pero puede causar fragmentación si no se fusionan los huecos vecinos.

---

#### **Optimizaciones**
1. **Listas separadas**:
    - Una lista para procesos y otra para huecos. Esto acelera la asignación pero complica la desasignación.
    - Si la lista de huecos se ordena por tamaño, se optimiza el mejor ajuste.
2. **Información embebida**:
    - En los huecos, se almacena directamente el tamaño y el puntero al siguiente nodo, eliminando la necesidad de estructuras adicionales.

#### Resumen
- **Mapas de bits** son simples y ocupan poca memoria, pero la búsqueda es lenta.
- **Listas libres** son más versátiles, especialmente con algoritmos como el primer o mejor ajuste, aunque pueden ser más complejas de manejar.
- La elección entre ambos métodos depende del balance entre rapidez, eficiencia de memoria y facilidad de implementación.


